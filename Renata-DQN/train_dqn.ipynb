{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The device being used is: cpu\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "# Setup\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"The device being used is: {device}\")\n",
        "\n",
        "# Constants\n",
        "NUCLEOTIDE_MAP = {\"A\": 1, \"T\": 2, \"C\": 3, \"G\": 4, \"-\": 5, \"PAD\": 0}\n",
        "NUCLEOTIDES = [\"PAD\", \"A\", \"T\", \"C\", \"G\", \"-\"]\n",
        "\n",
        "# config file info\n",
        "training_size = 1000\n",
        "max_nt_num = 150\n",
        "MAX_MSA_LEN = 50\n",
        "MAX_N_SEQS = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_epsilon_greedy(q_values: torch.Tensor, valid_mask: np.ndarray, epsilon: float, rng=None) -> int:\n",
        "    if q_values.ndim > 1:\n",
        "        q_values = q_values.reshape(-1)\n",
        "    valid_mask = np.asarray(valid_mask, dtype=bool)\n",
        "    \n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    \n",
        "    valid_idx = np.flatnonzero(valid_mask)\n",
        "    \n",
        "    if valid_idx.size == 0:\n",
        "        return int(torch.argmax(q_values).item())\n",
        "    \n",
        "    if rng.random() < epsilon:\n",
        "        return int(rng.choice(valid_idx))\n",
        "    \n",
        "    q = q_values.detach().cpu().numpy().copy()\n",
        "    q[~valid_mask] = -np.inf\n",
        "    return int(np.argmax(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_expected_alignment(sample):\n",
        "    for key in (\"solution\", \"aligned\", \"target\"):\n",
        "        if key in sample and sample[key] is not None:\n",
        "            result = sample[key]\n",
        "            if isinstance(result, str):\n",
        "                return [result]\n",
        "            return [str(s) for s in result]\n",
        "    return None\n",
        "\n",
        "'''def get_expected_gaps(sample):\n",
        "    if 'n_gaps' in sample and sample['n_gaps'] is not None:\n",
        "        print(\"using the n_gaps from sample\")\n",
        "        return sample['n_gaps']\n",
        "    \n",
        "    if 'moves' in sample and isinstance(sample['moves'], list):\n",
        "        print(\"using the moves from sample\")\n",
        "        return len(sample['moves'])\n",
        "    \n",
        "    if 'solution' in sample and sample['solution']:\n",
        "        if 'start' in sample:\n",
        "            start_gaps = sum(str(s).count('-') for s in sample['start'])\n",
        "            solution_gaps = sum(str(s).count('-') for s in sample['solution'])\n",
        "            return max(0, solution_gaps - start_gaps)\n",
        "        return sum(seq.count('-') for seq in sample['solution'])\n",
        "    \n",
        "    return 20 # why is this default? '''\n",
        "def get_expected_gaps(sample, scale_factor=1.2, min_gaps=5, max_gaps=150):\n",
        "    \"\"\"\n",
        "    Estimate expected number of gaps to insert during alignment, \n",
        "    without using ground-truth solution info.\n",
        "    \"\"\"\n",
        "    '''if 'n_gaps' in sample and sample['n_gaps'] is not None:\n",
        "        return int(sample['n_gaps'])'''\n",
        "\n",
        "    if 'start' in sample and isinstance(sample['start'], list):\n",
        "        lengths = [len(seq) for seq in sample['start']]\n",
        "        max_len = max(lengths)\n",
        "        total_missing = sum(max_len - l for l in lengths)\n",
        "        est_gaps = int(scale_factor * total_missing)\n",
        "        return int(np.clip(est_gaps, min_gaps, max_gaps))\n",
        "\n",
        "    return 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, memory_size=1000):\n",
        "        self.storage = []\n",
        "        self.memory_size = memory_size\n",
        "        self.size = 0\n",
        "    \n",
        "    def store(self, data: tuple):\n",
        "        if len(self.storage) == self.memory_size:\n",
        "            self.storage.pop(0)\n",
        "        self.storage.append(data)\n",
        "        self.size = min(self.size + 1, self.memory_size)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.storage, batch_size)\n",
        "        state = [s for s, _, _, _, _, _ in samples]\n",
        "        next_state = [ns for _, ns, _, _, _, _ in samples]\n",
        "        action = [a for _, _, a, _, _, _ in samples]\n",
        "        reward = [r for _, _, _, r, _, _ in samples]\n",
        "        done = [d for _, _, _, _, d, _ in samples]\n",
        "        next_mask = [m for _, _, _, _, _, m in samples]\n",
        "        return state, next_state, action, reward, done, next_mask\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "        return output, attn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_hid, n_position=200):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"pos_table\", self._get_sinusoid_table(n_position, d_hid))\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_sinusoid_table(n_position, d_hid):\n",
        "        positions = torch.arange(n_position).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_hid, 2).float() * (-np.log(10000.0) / d_hid))\n",
        "        sin = torch.sin(positions * div_term)\n",
        "        cos = torch.cos(positions * div_term)\n",
        "        return torch.cat([sin, cos], dim=-1).unsqueeze(0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pos_table[:, :x.size(1), :].clone().detach()\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.w_qs = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, d_v, bias=False)\n",
        "        self.fc = nn.Linear(d_v, d_model, bias=False)\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k**0.5)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "        residual = q\n",
        "        \n",
        "        q = self.w_qs(q).view(sz_b, len_q, 1, self.d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, 1, self.d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, 1, self.d_v)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "        q = self.layer_norm(q)\n",
        "        return q, attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_src_vocab, d_model, n_position, d_k=164, d_v=164, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_model, padding_idx=pad_idx)\n",
        "        self.position_enc = PositionalEncoding(d_model, n_position=n_position)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.self_attention = SelfAttention(d_model, d_k, d_v, dropout=dropout)\n",
        "    \n",
        "    def forward(self, src_seq, mask):\n",
        "        enc_output = self.src_word_emb(src_seq)\n",
        "        enc_output = self.position_enc(enc_output)\n",
        "        enc_output = self.dropout(enc_output)\n",
        "        enc_output = self.layer_norm(enc_output)\n",
        "        enc_output, _ = self.self_attention(enc_output, enc_output, enc_output, mask=mask)\n",
        "        return enc_output\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_sequences, max_sequence_length, num_actions, max_action_value, d_model=64):\n",
        "        super().__init__()\n",
        "        self.num_sequences = num_sequences\n",
        "        self.num_rows = num_sequences  # <-- no +2\n",
        "\n",
        "        dim = self.num_rows * max_sequence_length\n",
        "        # Encoder: your existing module that maps (B, dim) with an attention/MLP stack\n",
        "        self.encoder = Encoder(6, d_model, dim)\n",
        "\n",
        "        # Learnable row embeddings (still helpful)\n",
        "        self.seq_embedding = nn.Embedding(self.num_rows, d_model)\n",
        "        nn.init.normal_(self.seq_embedding.weight, 0.0, 0.1)\n",
        "\n",
        "        self.fc1 = nn.Linear(dim * d_model, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_actions)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, R, C = x.shape          # R == num_sequences\n",
        "        x_flat = x.view(B, R * C)\n",
        "        mask_flat = (x_flat != 0).unsqueeze(1)\n",
        "\n",
        "        h = self.encoder(x_flat, mask_flat)   # shape can be (B, R*C, d_model) or (B, d_model) per your Encoder\n",
        "        if h.dim() == 3:\n",
        "            h = h.view(B, R, C, -1)\n",
        "\n",
        "        # add per-row embedding (broadcast)\n",
        "        row_ids = torch.arange(R, device=h.device)\n",
        "        row_emb = self.seq_embedding(row_ids)[None, :, None, :]  # (1,R,1,d_model)\n",
        "        h = h + row_emb\n",
        "\n",
        "        h = h.reshape(B, -1)\n",
        "        h = F.leaky_relu(self.fc1(h)); h = self.dropout(h)\n",
        "        h = F.leaky_relu(self.fc2(h)); h = self.dropout(h)\n",
        "        q = self.fc3(h)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:    \n",
        "    def __init__(self, action_number, num_seqs, max_grid, max_value,\n",
        "                 epsilon=0.8, delta=0.05, decrement_iteration=5,\n",
        "                 update_iteration=128, batch_size=128, gamma=1.0,\n",
        "                 learning_rate=0.001, memory_size=1000):\n",
        "        self.seq_num = num_seqs\n",
        "        self.max_seq_len = max_grid  # <-- not +1\n",
        "        self.action_number = action_number\n",
        "\n",
        "        self.eval_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        \n",
        "        self.replay_memory = ReplayMemory(memory_size=memory_size)\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=learning_rate)\n",
        "        self.loss_func = nn.SmoothL1Loss()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.update_iteration = update_iteration\n",
        "        self.update_step_counter = 0\n",
        "        self.tau = 0.005\n",
        "        self.use_double_dqn = True\n",
        "        \n",
        "        self.initial_epsilon = epsilon\n",
        "        self.current_epsilon = epsilon\n",
        "        self.epsilon_end = delta\n",
        "        self.epsilon_decay = 0.999\n",
        "        \n",
        "        self.losses = []\n",
        "        self.epsilons = []\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.update_step_counter < 5000:\n",
        "            decay_rate = 0.9999\n",
        "        elif self.update_step_counter < 10000:\n",
        "            decay_rate = 0.999\n",
        "        else:\n",
        "            decay_rate = 0.995\n",
        "        \n",
        "        self.current_epsilon = max(self.epsilon_end, self.current_epsilon * decay_rate)\n",
        "        self.epsilons.append(self.current_epsilon)\n",
        "\n",
        "    def select_action(self, state, valid_action_mask=None):\n",
        "        is_random = (random.random() <= self.current_epsilon)\n",
        "        \n",
        "        if is_random:\n",
        "            if valid_action_mask is not None:\n",
        "                valid_idx = np.flatnonzero(valid_action_mask)\n",
        "                action = int(np.random.choice(valid_idx)) if len(valid_idx) else random.randrange(self.action_number)\n",
        "            else:\n",
        "                action = random.randrange(self.action_number)\n",
        "        else:\n",
        "            self.eval_net.eval()\n",
        "            with torch.no_grad():\n",
        "                s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num, self.max_seq_len)\n",
        "                q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "            self.eval_net.train()\n",
        "            \n",
        "            if valid_action_mask is not None:\n",
        "                q[~valid_action_mask] = -np.inf\n",
        "            \n",
        "            action = int(np.argmax(q))\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def predict(self, state, valid_action_mask=None):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num, self.max_seq_len)\n",
        "            q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "        self.eval_net.train()\n",
        "        \n",
        "        if valid_action_mask is not None:\n",
        "            q[~valid_action_mask] = -np.inf\n",
        "        \n",
        "        return int(np.nanargmax(q))\n",
        "\n",
        "    def forward(self, state):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(\n",
        "                1, self.seq_num, self.max_seq_len\n",
        "            )\n",
        "            q = self.eval_net(s).squeeze(0)\n",
        "        self.eval_net.train()\n",
        "        return q\n",
        "    \n",
        "    @property\n",
        "    def epsilon(self):\n",
        "        return self.current_epsilon\n",
        "\n",
        "    def update(self):\n",
        "        self.update_step_counter += 1\n",
        "        \n",
        "        if self.replay_memory.size < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        state, next_state, action, reward, done, next_mask = self.replay_memory.sample(self.batch_size)\n",
        "\n",
        "        batch_state = torch.LongTensor(state).to(device).view(-1, self.seq_num, self.max_seq_len)\n",
        "        batch_next_state = torch.LongTensor(next_state).to(device).view(-1, self.seq_num, self.max_seq_len)\n",
        "        batch_action = torch.LongTensor(action).unsqueeze(-1).to(device)\n",
        "        batch_reward = torch.FloatTensor(reward).unsqueeze(-1).to(device)\n",
        "        batch_done = torch.FloatTensor(done).unsqueeze(-1).to(device)\n",
        "        batch_next_mask = torch.BoolTensor(next_mask).to(device)\n",
        "        \n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if self.use_double_dqn:\n",
        "                q_next_online = self.eval_net(batch_next_state)\n",
        "                q_next_online_masked = q_next_online.clone()\n",
        "                q_next_online_masked[~batch_next_mask] = float('-inf')\n",
        "                best_next_actions = q_next_online_masked.max(1)[1]\n",
        "                \n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next = q_next_target.gather(1, best_next_actions.unsqueeze(1))\n",
        "            else:\n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next_masked = q_next_target.clone()\n",
        "                q_next_masked[~batch_next_mask] = float('-inf')\n",
        "                q_next = q_next_masked.max(1)[0].unsqueeze(-1)\n",
        "            \n",
        "            q_next = torch.where(torch.isinf(q_next), torch.zeros_like(q_next), q_next)\n",
        "            q_target = batch_reward + (1.0 - batch_done) * self.gamma * q_next\n",
        "            q_target = torch.clamp(q_target, -10.0, 10.0)\n",
        "        \n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.eval_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for target_param, online_param in zip(self.target_net.parameters(), self.eval_net.parameters()):\n",
        "                target_param.data.mul_(1.0 - self.tau)\n",
        "                target_param.data.add_(self.tau * online_param.data)\n",
        "        \n",
        "        self.losses.append(loss.item())\n",
        "        return loss.item()\n",
        "    \n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save the current model (eval_net) and target_net weights.\"\"\"\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save({\n",
        "            'eval_net_state_dict': self.eval_net.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.current_epsilon,\n",
        "            'update_step_counter': self.update_step_counter\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path, map_location=None):\n",
        "        \"\"\"Load model weights from a saved checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=map_location or device)\n",
        "        self.eval_net.load_state_dict(checkpoint['eval_net_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.current_epsilon = checkpoint.get('epsilon', self.current_epsilon)\n",
        "        self.update_step_counter = checkpoint.get('update_step_counter', 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example NUCLEOTIDE_MAP / NUCLEOTIDES assumed defined elsewhere:\n",
        "# NUCLEOTIDE_MAP = {'-':0,'A':1,'C':2,'G':3,'T':4}\n",
        "# NUCLEOTIDES = {v:k for k,v in NUCLEOTIDE_MAP.items()}\n",
        "\n",
        "class AlignmentEnvironment:\n",
        "    def __init__(self, sequences, total_gap, max_n_seqs=None, max_msa_len=None):\n",
        "        \"\"\"\n",
        "        consensus-free environment\n",
        "        sequences: list[str] (unaligned but will be padded)\n",
        "        total_gap: int (gap tokens the agent may insert)\n",
        "        \"\"\"\n",
        "        self.original_n_seqs = len(sequences)\n",
        "\n",
        "        # grid caps\n",
        "        self.max_n_seqs = max_n_seqs if max_n_seqs is not None else len(sequences)\n",
        "        self.max_msa_len = max_msa_len if max_msa_len is not None else max(len(s) for s in sequences)\n",
        "\n",
        "        # pad rows & pad #rows\n",
        "        sequences = self._pad_sequences(sequences, self.max_msa_len)\n",
        "        while len(sequences) < self.max_n_seqs:\n",
        "            sequences.append('-' * self.max_msa_len)\n",
        "\n",
        "        self.sequences = sequences\n",
        "        self.initial_sequences = copy.deepcopy(sequences)\n",
        "\n",
        "        # tokenized views\n",
        "        self.sep_nuc_in_seq = [[c for c in seq] for seq in sequences]\n",
        "        self.label_encoded_seqs = [[NUCLEOTIDE_MAP[c] for c in seq] for seq in sequences]\n",
        "\n",
        "        # numbers used by agent\n",
        "        self.num_seqs = self.max_n_seqs\n",
        "        self.action_number = self.max_n_seqs * self.max_msa_len\n",
        "\n",
        "        # rolling aligned/int arrays for state build (same as label_encoded_seqs initially)\n",
        "        self.aligned = [[] for _ in range(self.num_seqs)]\n",
        "        self.original = copy.deepcopy(self.label_encoded_seqs)\n",
        "\n",
        "        # rewards & gaps\n",
        "        self.last_score = self._pairwise_column_score(self.sep_nuc_in_seq[:self.original_n_seqs])\n",
        "        self.total_gap = total_gap\n",
        "        self.initial_gap = total_gap\n",
        "        self.gaps_per_sequence = [0] * self.num_seqs\n",
        "\n",
        "        # bookkeeping\n",
        "        self.recent_actions = []\n",
        "        self.last_seq_chosen = None\n",
        "        self.seq_streak_count = 0\n",
        "        self.seq_permutation = None\n",
        "        self.seq_permutation_inv = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad_sequences(sequences, target_len):\n",
        "        return [s.ljust(target_len, '-') for s in sequences]\n",
        "\n",
        "    def get_valid_action_mask(self):\n",
        "        \"\"\"\n",
        "        Valid action: placing a gap where the current character is not already '-'.\n",
        "        Action space: index = seq_idx * max_msa_len + pos\n",
        "        \"\"\"\n",
        "        A = self.action_number\n",
        "        mask = np.zeros(A, dtype=bool)\n",
        "        for seq_idx in range(self.original_n_seqs):\n",
        "            row = self.sequences[seq_idx]\n",
        "            for pos in range(self.max_msa_len):\n",
        "                action = seq_idx * self.max_msa_len + pos\n",
        "                mask[action] = (row[pos] != '-')\n",
        "        return mask\n",
        "\n",
        "    # ---------- CONSENSUS-FREE REWARD ----------\n",
        "    def _pairwise_column_score(self, rows, bonus_perfect=1.15):\n",
        "        \"\"\"\n",
        "        Sum over columns:\n",
        "          - For each column, count majority agreement among non-gaps (sum-of-pairs proxy).\n",
        "          - If all non-gap residues in the column are identical (and at least one non-gap), multiply that column's contribution by bonus_perfect.\n",
        "        \"\"\"\n",
        "        if not rows:\n",
        "            return 0.0\n",
        "        n = len(rows)\n",
        "        L = min(len(r) for r in rows)\n",
        "        score = 0.0\n",
        "        for c in range(L):\n",
        "            col = [rows[r][c] for r in range(n)]\n",
        "            nz = [b for b in col if b != '-']\n",
        "            if not nz:\n",
        "                continue\n",
        "            # sum-of-pairs proxy: majority size\n",
        "            majority = max(sum(b == base for b in nz) for base in set(nz))\n",
        "            col_score = majority\n",
        "            # perfect non-gap column bonus\n",
        "            if len(set(nz)) == 1:\n",
        "                col_score *= bonus_perfect\n",
        "            score += col_score\n",
        "        return float(score)\n",
        "\n",
        "    def get_current_state(self):\n",
        "        \"\"\"\n",
        "        Return a 1-D list of ints with shape (num_seqs * max_msa_len).\n",
        "        (No more +2 consensus rows.)\n",
        "        \"\"\"\n",
        "        state = []\n",
        "        for seq in self.original:\n",
        "            state.extend(seq)\n",
        "        return state\n",
        "\n",
        "    # Predicted-only metrics (SP/CS) for validation & optional reward shaping\n",
        "    def _predicted_sp_score(self, msa):\n",
        "        # +2 match (non-gap), -1 if any gap, -2 mismatch (non-gap)\n",
        "        n = len(msa)\n",
        "        if n == 0: return 0.0\n",
        "        L = len(msa[0])\n",
        "        assert all(len(s) == L for s in msa)\n",
        "        score = 0\n",
        "        from itertools import combinations\n",
        "        for i, j in combinations(range(n), 2):\n",
        "            si, sj = msa[i], msa[j]\n",
        "            for c in range(L):\n",
        "                a, b = si[c], sj[c]\n",
        "                if a == '-' or b == '-':\n",
        "                    score -= 1\n",
        "                elif a == b:\n",
        "                    score += 2\n",
        "                else:\n",
        "                    score -= 2\n",
        "        return float(score)\n",
        "\n",
        "    def _predicted_cs_fraction(self, msa):\n",
        "        n = len(msa)\n",
        "        if n == 0: return 0.0\n",
        "        L = len(msa[0])\n",
        "        assert all(len(s) == L for s in msa)\n",
        "        good = 0\n",
        "        for c in range(L):\n",
        "            col = [msa[r][c] for r in range(n)]\n",
        "            nz = [x for x in col if x != '-']\n",
        "            if nz and len(set(nz)) == 1:\n",
        "                good += 1\n",
        "        return good / L if L > 0 else 0.0\n",
        "\n",
        "    # Public calc for validation “reward-like” score\n",
        "    def calc_reward_from_alignment(self, aligned_list_of_str):\n",
        "        rows = [[ch for ch in row] for row in aligned_list_of_str[:self.original_n_seqs]]\n",
        "        return self._pairwise_column_score(rows)\n",
        "\n",
        "    def calc_reward(self):\n",
        "        \"\"\"Current alignment score of internal sequences (for validation prints).\"\"\"\n",
        "        rows = [[ch for ch in row] for row in self.sequences[:self.original_n_seqs]]\n",
        "        return self._pairwise_column_score(rows)\n",
        "\n",
        "    # ----------- STEP -----------\n",
        "    def step(self, action):\n",
        "        seq_idx = int(action) // self.max_msa_len\n",
        "        pos = int(action) % self.max_msa_len\n",
        "\n",
        "        if not (0 <= seq_idx < self.original_n_seqs and 0 <= pos < self.max_msa_len):\n",
        "            # invalid action\n",
        "            reward = -1.0\n",
        "            return reward, self.get_current_state(), 0\n",
        "\n",
        "        row = list(self.sequences[seq_idx])\n",
        "        placed = False\n",
        "        if row[pos] != '-':\n",
        "            row.insert(pos, '-')        # shift right from pos\n",
        "            row.pop()                   # keep row length fixed\n",
        "            self.sequences[seq_idx] = \"\".join(row)\n",
        "\n",
        "            # sync token views\n",
        "            self.sep_nuc_in_seq[seq_idx]     = row\n",
        "            self.label_encoded_seqs[seq_idx] = [NUCLEOTIDE_MAP[c] for c in row]\n",
        "            self.original[seq_idx]           = [NUCLEOTIDE_MAP[c] for c in row]\n",
        "\n",
        "            self.gaps_per_sequence[seq_idx] += 1\n",
        "            self.total_gap = max(0, self.total_gap - 1)\n",
        "            placed = True\n",
        "\n",
        "        # compute dense local reward\n",
        "        #  - small step reward/penalty\n",
        "        #  - add terminal bonus based on entropy/diversity if you kept those helpers\n",
        "        reward = 0.0\n",
        "        reward += 0.5 if placed else -0.1\n",
        "\n",
        "        done = 1 if self.total_gap == 0 else 0\n",
        "        if done == 1:\n",
        "            # optional: add diversity term if you kept it\n",
        "            # reward += self._diversity_entropy(scale=20.0)\n",
        "            pass\n",
        "\n",
        "        reward = float(np.clip(reward, -25.0, 25.0))\n",
        "        return reward, self.get_current_state(), done\n",
        "\n",
        "    # -------- Reset / Permutation (unchanged) --------\n",
        "    def get_alignment(self):\n",
        "        alignment = []\n",
        "        for i in range(len(self.aligned)):\n",
        "            alignment.append(''.join([NUCLEOTIDES[self.aligned[i][j]] for j in range(len(self.aligned[i]))]))\n",
        "        return alignment\n",
        "\n",
        "    def get_original_alignment(self):\n",
        "        # returns only real sequences, up to grid width\n",
        "        full_alignment = self.sequences\n",
        "        return [full_alignment[i] for i in range(self.original_n_seqs)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.aligned = [[] for _ in range(self.num_seqs)]\n",
        "        self.original = copy.deepcopy(self.label_encoded_seqs)\n",
        "        self.total_gap = self.initial_gap\n",
        "        self.last_score = self._pairwise_column_score(self.sep_nuc_in_seq[:self.original_n_seqs])\n",
        "        self.gaps_per_sequence = [0] * self.num_seqs\n",
        "        self.sequences = copy.deepcopy(self.initial_sequences)\n",
        "        self.sep_nuc_in_seq = [[c for c in seq] for seq in self.sequences]\n",
        "        self.recent_actions = []\n",
        "        self.last_seq_chosen = None\n",
        "        self.seq_streak_count = 0\n",
        "        self.seq_permutation = None\n",
        "        self.seq_permutation_inv = None\n",
        "        return self.get_current_state()\n",
        "\n",
        "    def randomize_sequence_order(self, apply=True):\n",
        "        if not apply or self.original_n_seqs <= 1:\n",
        "            self.seq_permutation = None\n",
        "            self.seq_permutation_inv = None\n",
        "            return\n",
        "        self.seq_permutation = np.random.permutation(self.original_n_seqs)\n",
        "        self.seq_permutation_inv = np.argsort(self.seq_permutation)\n",
        "        self.original[:self.original_n_seqs]        = [self.original[i]        for i in self.seq_permutation]\n",
        "        self.sequences[:self.original_n_seqs]       = [self.sequences[i]       for i in self.seq_permutation]\n",
        "        self.sep_nuc_in_seq[:self.original_n_seqs]  = [self.sep_nuc_in_seq[i]  for i in self.seq_permutation]\n",
        "        self.label_encoded_seqs[:self.original_n_seqs] = [self.label_encoded_seqs[i] for i in self.seq_permutation]\n",
        "        gaps_copy = self.gaps_per_sequence[:self.original_n_seqs].copy()\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            self.gaps_per_sequence[new_idx] = gaps_copy[old_idx]\n",
        "\n",
        "    \n",
        "    def get_unpermuted_gaps(self):\n",
        "        if self.seq_permutation_inv is None:\n",
        "            return self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        \n",
        "        original_order = [0] * self.original_n_seqs\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            original_order[old_idx] = self.gaps_per_sequence[new_idx]\n",
        "        return original_order\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_sp_score(pred, match=2, mismatch=-2, gap = -1):\n",
        "        n = len(pred)\n",
        "        L = len(pred[0])\n",
        "\n",
        "        score = 0\n",
        "        for i, j in combinations(range(n), 2):\n",
        "            si, sj = pred[i], pred[j]\n",
        "            for c in range(L):\n",
        "                a, b = si[c], sj[c]\n",
        "                if a == '-' and b == '-':\n",
        "                    score += 0 \n",
        "                elif a == '-' or b == '-':\n",
        "                    score += gap\n",
        "                elif a == b:\n",
        "                    score += match\n",
        "                else:\n",
        "                    score += mismatch\n",
        "        return float(score)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_cs_score(pred):\n",
        "        n = len(pred)\n",
        "        L = len(pred[0])\n",
        "\n",
        "        good = 0\n",
        "        for c in range(L):\n",
        "            col = [pred[r][c] for r in range(n)]\n",
        "            nz = [x for x in col if x != '-']      # non-gap residues\n",
        "            if len(nz) == 0:\n",
        "                # no residues in this column → by definition not a “matching residues” column\n",
        "                continue\n",
        "            if len(set(nz)) == 1:\n",
        "                good += 1\n",
        "        return good / L if L > 0 else 0.0\n",
        "    \n",
        "    @staticmethod\n",
        "    def compute_alignment_metrics(pred, ref):\n",
        "        \"\"\"\n",
        "        Computes predicted-only scores (SP, CS) and reference-based metrics (Q*, TC*).\n",
        "\n",
        "        Q(A,R): pair-based accuracy (sum-of-pairs)\n",
        "        TC(A,R): column-based accuracy (total-column match)\n",
        "\n",
        "        Columns are compared by tuples (bases_across_sequences, column_index)\n",
        "        so the logic is parallel to the pair-based comparison.\n",
        "        \"\"\"\n",
        "        assert len(pred) == len(ref), \"Pred/ref must have same number of sequences.\"\n",
        "        n = len(ref)\n",
        "        if n == 0:\n",
        "            return {k: 0.0 for k in [\n",
        "                'pred_sp','pred_cs','Q_acc','Q_prec','Q_rec','Q_f1',\n",
        "                'TC_acc','TC_prec','TC_rec','TC_f1'\n",
        "            ]}\n",
        "        Lp = len(pred[0])\n",
        "        Lr = len(ref[0])\n",
        "        assert all(len(s) == Lp for s in pred)\n",
        "        assert all(len(s) == Lr for s in ref)\n",
        "        L = min(Lp, Lr)\n",
        "\n",
        "        # --- predicted-only metrics\n",
        "        pred_sp = AlignmentEnvironment.get_sp_score(pred)\n",
        "        pred_cs = AlignmentEnvironment.get_cs_score(pred)\n",
        "\n",
        "        # ---------- Q (pair) metrics ----------\n",
        "        def get_pairs(msa):\n",
        "            pairs = set()\n",
        "            for i, j in combinations(range(n), 2):\n",
        "                seq_i, seq_j = msa[i], msa[j]\n",
        "                for c in range(L):\n",
        "                    a, b = seq_i[c], seq_j[c]\n",
        "                    if a != '-' and b != '-':\n",
        "                        pairs.add((i, j, c))\n",
        "            return pairs\n",
        "\n",
        "        pred_pairs = get_pairs(pred)\n",
        "        ref_pairs = get_pairs(ref)\n",
        "\n",
        "        TPp = len(pred_pairs & ref_pairs)\n",
        "        FPp = len(pred_pairs - ref_pairs)\n",
        "        FNp = len(ref_pairs - pred_pairs)\n",
        "\n",
        "        Q_acc  = TPp / len(ref_pairs) if len(ref_pairs) > 0 else 0.0\n",
        "        Q_prec = TPp / (TPp + FPp) if (TPp + FPp) > 0 else 0.0\n",
        "        Q_rec  = TPp / (TPp + FNp) if (TPp + FNp) > 0 else 0.0\n",
        "        Q_f1   = (2 * Q_prec * Q_rec / (Q_prec + Q_rec)) if (Q_prec + Q_rec) > 0 else 0.0\n",
        "\n",
        "        # ---------- TC (column) metrics ----------\n",
        "        def get_columns(msa):\n",
        "            cols = set()\n",
        "            for c in range(L):\n",
        "                col = tuple(msa[r][c] for r in range(n))\n",
        "                cols.add((c, col))  # include index for uniqueness\n",
        "            return cols\n",
        "\n",
        "        pred_cols = get_columns(pred)\n",
        "        ref_cols  = get_columns(ref)\n",
        "\n",
        "        TPc = len(pred_cols & ref_cols)\n",
        "        FPc = len(pred_cols - ref_cols)\n",
        "        FNc = len(ref_cols - pred_cols)\n",
        "\n",
        "        TC_acc  = TPc / len(ref_cols) if len(ref_cols) > 0 else 0.0\n",
        "        TC_prec = TPc / (TPc + FPc) if (TPc + FPc) > 0 else 0.0\n",
        "        TC_rec  = TPc / (TPc + FNc) if (TPc + FNc) > 0 else 0.0\n",
        "        TC_f1   = (2 * TC_prec * TC_rec / (TC_prec + TC_rec)) if (TC_prec + TC_rec) > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            \"pred_sp\": float(pred_sp),\n",
        "            \"pred_cs\": float(pred_cs),\n",
        "            \"Q_acc\": float(Q_acc),\n",
        "            \"Q_prec\": float(Q_prec),\n",
        "            \"Q_rec\": float(Q_rec),\n",
        "            \"Q_f1\": float(Q_f1),\n",
        "            \"TC_acc\": float(TC_acc),\n",
        "            \"TC_prec\": float(TC_prec),\n",
        "            \"TC_rec\": float(TC_rec),\n",
        "            \"TC_f1\": float(TC_f1)\n",
        "        }\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_prediction_episode(env, agent, expected_gaps, max_factor=1.25):\n",
        "    import math\n",
        "    import numpy as np\n",
        "    \n",
        "    state = env.reset()\n",
        "    max_steps = max(1, int(math.ceil(expected_gaps * max_factor)))\n",
        "    steps, rewards = 0, []\n",
        "    \n",
        "    for t in range(max_steps):\n",
        "        valid_mask = env.get_valid_action_mask().astype(bool)\n",
        "        \n",
        "        q_values = agent.forward(state)\n",
        "        action = masked_epsilon_greedy(\n",
        "            q_values=q_values,\n",
        "            valid_mask=valid_mask,\n",
        "            epsilon=getattr(agent, \"epsilon\", 0.1),\n",
        "        )\n",
        "        \n",
        "        reward, next_state, done = env.step(action)\n",
        "        rewards.append(float(reward))\n",
        "        \n",
        "        if hasattr(agent, \"replay_memory\"):\n",
        "            agent.replay_memory.store(\n",
        "                (state, next_state, action, reward, bool(done), valid_mask.copy())\n",
        "            )\n",
        "        \n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        \n",
        "        if done == 1 or not valid_mask.any():\n",
        "            break\n",
        "    \n",
        "    gaps_dist = env.get_unpermuted_gaps() if hasattr(env, \"get_unpermuted_gaps\") \\\n",
        "                else env.gaps_per_sequence[:env.original_n_seqs]\n",
        "    \n",
        "    return {\n",
        "        \"steps\": steps,\n",
        "        \"avg_reward\": float(np.mean(rewards)) if rewards else 0.0,\n",
        "        \"sum_reward\": float(np.sum(rewards)) if rewards else 0.0,\n",
        "        \"gaps_distribution\": gaps_dist,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 1494999/1494999 [00:07<00:00, 189521.61 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'start': ['TACTACAGTTCTTAAAAATAATCTATTAAAATTTTTTTGCT', 'TAGTACGATTCGTGAAAATAATCTGTTAAAATTCTTTTTCT', 'TTATACAATTTTTTGAGGATTAATCTGTTGAAATTATTGTTCT'], 'solution': ['TACTACAGTTCTT--AAAAATAATCTATTAAAATTTTTTTGCT', 'TAGTACGATTCGT--GAAAATAATCTGTTAAAATTCTTTTTCT', 'TTATACAATTTTTTGAGGATTAATCTGTTGAAATTATTGTTCT'], 'n_gaps': 4, 'moves': [-1, -1, -1, -1], 'n_sequences': 3, 'idx': 0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 118644.04 examples/s]\n",
            "Filter: 100%|██████████| 3001/3001 [00:00<00:00, 91076.94 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def _count_inserted_gaps_from_sequences(start, solution):\n",
        "    dash_start = sum(str(s).count('-') for s in start)\n",
        "    dash_solution = sum(str(s).count('-') for s in solution)\n",
        "    return max(0, dash_solution - dash_start)\n",
        "\n",
        "def convert_column_major_solution(msa_string, n_seq):\n",
        "    \"\"\"\n",
        "    Converts a column-major MSA string (down columns first) into\n",
        "    a row-major list of aligned sequences.\n",
        "    \n",
        "    Args:\n",
        "        msa_string (str): e.g. \"AAACC---CGGTTTT\"\n",
        "        n_seq (int): number of sequences (rows)\n",
        "    \n",
        "    Returns:\n",
        "        list[str]: e.g. [\"ACGT-\", \"A-GT-\", \"AC-T-\"]\n",
        "    \"\"\"\n",
        "    if not msa_string or n_seq <= 0:\n",
        "        return []\n",
        "\n",
        "    # Split into chunks of n_seq (each chunk = one column)\n",
        "    columns = [msa_string[i:i+n_seq] for i in range(0, len(msa_string), n_seq)]\n",
        "\n",
        "    # Transpose columns -> rows\n",
        "    seqs = [''.join(col[i] for col in columns) for i in range(n_seq)]\n",
        "    return seqs\n",
        "\n",
        "def convert_huggingface_to_samples(dataset, max_samples=None):\n",
        "    samples = []\n",
        "    for i, ex in enumerate(dataset):\n",
        "        if max_samples and i >= max_samples:\n",
        "            break\n",
        "\n",
        "        unaligned_seqs = ex.get('unaligned_seqs', {})\n",
        "        MSA = ex.get('MSA', \"\")\n",
        "\n",
        "        if not unaligned_seqs or not MSA:\n",
        "            continue\n",
        "\n",
        "        start = [unaligned_seqs[k] for k in sorted(unaligned_seqs.keys())]\n",
        "        n_seq = len(start)\n",
        "        solution = convert_column_major_solution(MSA, n_seq)\n",
        "\n",
        "        accepted_pairs = [(str(a), str(b)) for a, b in combinations(range(len(start)), 2)]\n",
        "        n_gaps = _count_inserted_gaps_from_sequences(start, solution)\n",
        "\n",
        "        sample = {\n",
        "            'start': start,\n",
        "            'solution': solution,\n",
        "            'n_gaps': n_gaps,\n",
        "            'moves': [-1] * n_gaps,  # keep list length equal to n_gaps as this is never actually used in the DQN\n",
        "            'n_sequences': len(start),\n",
        "            'idx': i\n",
        "        }\n",
        "        samples.append(sample)\n",
        "    return samples\n",
        "\n",
        "\n",
        "def filter_by_seq_length(example, max_len=MAX_MSA_LEN):\n",
        "    \"\"\"Keep only samples where every unaligned sequence is <= max_len.\"\"\"\n",
        "    if \"unaligned_seqs\" not in example:\n",
        "        return False\n",
        "    seqs = example[\"unaligned_seqs\"].values() if isinstance(example[\"unaligned_seqs\"], dict) else example[\"unaligned_seqs\"]\n",
        "    return all(len(seq) <= max_len for seq in seqs)\n",
        "\n",
        "# --- Load and filter datasets ---\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"train\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "train_samples = convert_huggingface_to_samples(ds, max_samples=training_size)\n",
        "print(train_samples[0])\n",
        "\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"validation\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "val_samples = convert_huggingface_to_samples(ds)\n",
        "\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"test\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "test_samples = convert_huggingface_to_samples(ds) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Dimension Analysis (Using ALL Data with Padding) DEPRECATEDDDDDDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n_seqs = max(s['n_sequences'] for s in train_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in train_samples)\n",
        "\n",
        "for sample in train_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n",
        "\n",
        "max_n_seqs = max(s['n_sequences'] for s in val_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in val_samples)\n",
        "\n",
        "for sample in val_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n",
        "\n",
        "max_n_seqs = max(s['n_sequences'] for s in test_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in test_samples)\n",
        "for sample in test_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training with Variable Dimensions (Using Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 5/5 [00:00<00:00, 107.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 13 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 2 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 40 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 2 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 0 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "Epoch 1: reward=5.70, memory=57, ε=0.900\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "Epoch 1: SP=130.80, CS=0.580, Q_acc=0.961, Q_prec=0.914, Q_rec=0.961, Q_f1=0.936, TC_acc=0.116, TC_prec=0.116, TC_rec=0.116, TC_f1=0.116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:   0%|          | 0/5 [00:00<?, ?it/s]/var/folders/np/3n8gblwj5054scn97qw91dpc0000gn/T/ipykernel_66218/1541028366.py:106: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  batch_next_mask = torch.BoolTensor(next_mask).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 6 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 5 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  40%|████      | 2/5 [00:02<00:03,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 33 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  60%|██████    | 3/5 [00:17<00:13,  6.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 6 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  80%|████████  | 4/5 [00:20<00:05,  5.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "The env setup is initialized with 4 gaps, max_n_seqs=3\n",
            "using the n_gaps from sample\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 5/5 [00:22<00:00,  4.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: reward=5.40, memory=111, ε=0.899\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "using the n_gaps from sample\n",
            "Epoch 2: SP=-58.80, CS=0.136, Q_acc=0.868, Q_prec=0.992, Q_rec=0.868, Q_f1=0.923, TC_acc=0.014, TC_prec=0.014, TC_rec=0.014, TC_f1=0.014\n",
            "\n",
            "Training complete. Model saved to: ../result/agent/2025-11-09_02-04-25_model.pt\n",
            "Training log saved to: ../result/log/2025-11-09_02-04-25_log.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Create results directory ---\n",
        "os.makedirs(\"../result/log\", exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "log_path = os.path.join(\"../result/log\", f\"{timestamp}_log.csv\")\n",
        "\n",
        "# --- Prepare CSV logging ---\n",
        "log_fields = [\n",
        "    \"epoch\", \"avg_train_reward\", \"epsilon\",\n",
        "    \"SP\", \"CS\", \"Q_acc\", \"Q_prec\", \"Q_rec\", \"Q_f1\", \"TC_acc\", \"TC_prec\", \"TC_rec\", \"TC_f1\"\n",
        "]\n",
        "\n",
        "with open(log_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=log_fields)\n",
        "    writer.writeheader()\n",
        "\n",
        "max_n_seqs = MAX_N_SEQS\n",
        "agent = DQNAgent(\n",
        "    action_number=max_n_seqs * MAX_MSA_LEN,\n",
        "    num_seqs=max_n_seqs,\n",
        "    max_grid=MAX_MSA_LEN,           # not consensus length\n",
        "    max_value=MAX_MSA_LEN * 100,\n",
        "    epsilon=0.9,\n",
        "    delta=0.01,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    learning_rate=0.001,\n",
        "    memory_size=5000\n",
        ")\n",
        "\n",
        "epochs = 2 # 10\n",
        "samples_per_epoch = 5 # 100\n",
        "val_samples_per_epoch = 5 # 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_samples = random.sample(train_samples, min(len(train_samples), samples_per_epoch))\n",
        "    epoch_rewards = []\n",
        "    \n",
        "    for sample in tqdm(epoch_samples, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        env = AlignmentEnvironment(\n",
        "            sequences=sample['start'],               # no accepted_pairs\n",
        "            total_gap=get_expected_gaps(sample),     # your existing helper\n",
        "            max_n_seqs=max_n_seqs,\n",
        "            max_msa_len=MAX_MSA_LEN\n",
        "        )\n",
        "        print(f\"The env setup is initialized with {get_expected_gaps(sample)} gaps, max_n_seqs={max_n_seqs}\")\n",
        "        \n",
        "        state = env.reset()\n",
        "        env.randomize_sequence_order(apply=True)\n",
        "        \n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "        max_steps = get_expected_gaps(sample) * 5\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "        \n",
        "        while step_count < max_steps:\n",
        "            action = agent.select_action(state, valid_action_mask=valid_mask)\n",
        "            reward, next_state, done = env.step(action)\n",
        "            \n",
        "            next_valid_mask = env.get_valid_action_mask()\n",
        "            agent.replay_memory.store((state, next_state, action, reward, done, next_valid_mask))\n",
        "            \n",
        "            agent.update()\n",
        "            \n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "            state = next_state\n",
        "            valid_mask = next_valid_mask\n",
        "            \n",
        "            if done == 1:\n",
        "                break\n",
        "        \n",
        "        agent.update_epsilon()\n",
        "        epoch_rewards.append(episode_reward)\n",
        "    \n",
        "    avg_train_reward = np.mean(epoch_rewards)\n",
        "    print(f\"Epoch {epoch+1}: reward={avg_train_reward:.2f}, memory={agent.replay_memory.size}, ε={agent.current_epsilon:.3f}\")\n",
        "\n",
        "    # Validation section\n",
        "    val_metrics_all = {\n",
        "        \"pred_sp\": [], \"pred_cs\": [],\n",
        "        \"Q_acc\": [], \"Q_prec\": [], \"Q_rec\": [], \"Q_f1\": [],\n",
        "        \"TC_acc\": [], \"TC_prec\": [], \"TC_rec\": [], \"TC_f1\": []\n",
        "    }\n",
        "\n",
        "    for val_sample in random.sample(val_samples, min(val_samples_per_epoch, len(val_samples))):\n",
        "        env = AlignmentEnvironment(\n",
        "            sequences=sample['start'],               # no accepted_pairs\n",
        "            total_gap=get_expected_gaps(sample),     # your existing helper\n",
        "            max_n_seqs=max_n_seqs,\n",
        "            max_msa_len=MAX_MSA_LEN\n",
        "        )\n",
        "\n",
        "        predicted, _ = run_dqn_inference(agent, env, get_expected_gaps(val_sample))\n",
        "\n",
        "        metrics = env.compute_alignment_metrics(predicted, val_sample['solution'])\n",
        "        val_reward = env.calc_reward()\n",
        "\n",
        "        # Aggregate metrics\n",
        "        for k, v in metrics.items():\n",
        "            val_metrics_all[k].append(v)\n",
        "\n",
        "\n",
        "    # --- Average metrics across all validation samples ---\n",
        "    avg_val_metrics = {k: np.mean(v) if len(v) > 0 else 0.0 for k, v in val_metrics_all.items()}\n",
        "\n",
        "    # --- Print epoch summary ---\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}: \"\n",
        "        f\"SP={avg_val_metrics['pred_sp']:.2f}, \"\n",
        "        f\"CS={avg_val_metrics['pred_cs']:.3f}, \"\n",
        "        f\"Q_acc={avg_val_metrics['Q_acc']:.3f}, \"\n",
        "        f\"Q_prec={avg_val_metrics['Q_prec']:.3f}, \"\n",
        "        f\"Q_rec={avg_val_metrics['Q_rec']:.3f}, \"\n",
        "        f\"Q_f1={avg_val_metrics['Q_f1']:.3f}, \"\n",
        "        f\"TC_acc={avg_val_metrics['TC_acc']:.3f}, \"\n",
        "        f\"TC_prec={avg_val_metrics['TC_prec']:.3f}, \"\n",
        "        f\"TC_rec={avg_val_metrics['TC_rec']:.3f}, \"\n",
        "        f\"TC_f1={avg_val_metrics['TC_f1']:.3f}\"\n",
        "    )\n",
        "\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
        "        writer.writerow({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"avg_train_reward\": avg_train_reward,\n",
        "            \"epsilon\": agent.current_epsilon,\n",
        "            \"SP\": avg_val_metrics['pred_sp'],\n",
        "            \"CS\": avg_val_metrics['pred_cs'],\n",
        "            \"Q_acc\": avg_val_metrics['Q_acc'],\n",
        "            \"Q_prec\": avg_val_metrics['Q_prec'],\n",
        "            \"Q_rec\": avg_val_metrics['Q_rec'],\n",
        "            \"Q_f1\": avg_val_metrics['Q_f1'],\n",
        "            \"TC_acc\": avg_val_metrics['TC_acc'],\n",
        "            \"TC_prec\": avg_val_metrics['TC_prec'],\n",
        "            \"TC_rec\": avg_val_metrics['TC_rec'],\n",
        "            \"TC_f1\": avg_val_metrics['TC_f1']\n",
        "        })\n",
        "\n",
        "# --- Save trained model ---\n",
        "os.makedirs(\"../result/agent\", exist_ok=True)\n",
        "model_path = os.path.join(\"../result/agent\", f\"{timestamp}_model.pt\")\n",
        "agent.save_model(model_path)\n",
        "print(f\"\\nTraining complete. Model saved to: {model_path}\")\n",
        "print(f\"Training log saved to: {log_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_dqn_inference(agent, env, expected_gaps, max_steps=None):\n",
        "    \"\"\"\n",
        "    Runs an inference episode using the trained DQN agent in the consensus-free environment.\n",
        "    \"\"\"\n",
        "    if max_steps is None:\n",
        "        max_steps = expected_gaps * 2\n",
        "\n",
        "    state = env.reset()\n",
        "    env.randomize_sequence_order(apply=True)\n",
        "\n",
        "    actions_taken = []\n",
        "    valid_mask = env.get_valid_action_mask()\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if valid_mask.sum() == 0:\n",
        "            break\n",
        "\n",
        "        # Predict next action\n",
        "        action = agent.predict(state, valid_action_mask=valid_mask)\n",
        "\n",
        "        # Convert linear action index to (sequence, position)\n",
        "        seq_idx = action // env.max_msa_len\n",
        "        pos = action % env.max_msa_len\n",
        "        actions_taken.append((seq_idx, pos))\n",
        "\n",
        "        # Apply action in environment\n",
        "        _, next_state, done = env.step(action)\n",
        "        state = next_state\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "\n",
        "        if done == 1:\n",
        "            break\n",
        "\n",
        "    predicted = env.sequences[:env.original_n_seqs]\n",
        "\n",
        "    return predicted, actions_taken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "COMP545",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
