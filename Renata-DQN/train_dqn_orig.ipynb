{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Setup\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Constants\n",
        "NUCLEOTIDE_MAP = {\"A\": 1, \"T\": 2, \"C\": 3, \"G\": 4, \"-\": 5, \"PAD\": 0}\n",
        "NUCLEOTIDES = [\"PAD\", \"A\", \"T\", \"C\", \"G\", \"-\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def masked_epsilon_greedy(q_values: torch.Tensor, valid_mask: np.ndarray, epsilon: float, rng=None) -> int:\n",
        "    if q_values.ndim > 1:\n",
        "        q_values = q_values.reshape(-1)\n",
        "    valid_mask = np.asarray(valid_mask, dtype=bool)\n",
        "    \n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    \n",
        "    valid_idx = np.flatnonzero(valid_mask)\n",
        "    \n",
        "    if valid_idx.size == 0:\n",
        "        return int(torch.argmax(q_values).item())\n",
        "    \n",
        "    if rng.random() < epsilon:\n",
        "        return int(rng.choice(valid_idx))\n",
        "    \n",
        "    q = q_values.detach().cpu().numpy().copy()\n",
        "    q[~valid_mask] = -np.inf\n",
        "    return int(np.argmax(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_expected_alignment(sample):\n",
        "    for key in (\"solution\", \"aligned\", \"target\"):\n",
        "        if key in sample and sample[key] is not None:\n",
        "            result = sample[key]\n",
        "            if isinstance(result, str):\n",
        "                return [result]\n",
        "            return [str(s) for s in result]\n",
        "    return None\n",
        "\n",
        "def get_expected_gaps(sample):\n",
        "    if 'n_gaps' in sample and sample['n_gaps'] is not None:\n",
        "        return sample['n_gaps']\n",
        "    \n",
        "    if 'moves' in sample and isinstance(sample['moves'], list):\n",
        "        return len(sample['moves'])\n",
        "    \n",
        "    if 'solution' in sample and sample['solution']:\n",
        "        if 'start' in sample:\n",
        "            start_gaps = sum(str(s).count('-') for s in sample['start'])\n",
        "            solution_gaps = sum(str(s).count('-') for s in sample['solution'])\n",
        "            return max(0, solution_gaps - start_gaps)\n",
        "        return sum(seq.count('-') for seq in sample['solution'])\n",
        "    \n",
        "    return 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, memory_size=1000):\n",
        "        self.storage = []\n",
        "        self.memory_size = memory_size\n",
        "        self.size = 0\n",
        "    \n",
        "    def store(self, data: tuple):\n",
        "        if len(self.storage) == self.memory_size:\n",
        "            self.storage.pop(0)\n",
        "        self.storage.append(data)\n",
        "        self.size = min(self.size + 1, self.memory_size)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.storage, batch_size)\n",
        "        state = [s for s, _, _, _, _, _ in samples]\n",
        "        next_state = [ns for _, ns, _, _, _, _ in samples]\n",
        "        action = [a for _, _, a, _, _, _ in samples]\n",
        "        reward = [r for _, _, _, r, _, _ in samples]\n",
        "        done = [d for _, _, _, _, d, _ in samples]\n",
        "        next_mask = [m for _, _, _, _, _, m in samples]\n",
        "        return state, next_state, action, reward, done, next_mask\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "        return output, attn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_hid, n_position=200):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"pos_table\", self._get_sinusoid_table(n_position, d_hid))\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_sinusoid_table(n_position, d_hid):\n",
        "        positions = torch.arange(n_position).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_hid, 2).float() * (-np.log(10000.0) / d_hid))\n",
        "        sin = torch.sin(positions * div_term)\n",
        "        cos = torch.cos(positions * div_term)\n",
        "        return torch.cat([sin, cos], dim=-1).unsqueeze(0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pos_table[:, :x.size(1), :].clone().detach()\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.w_qs = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, d_v, bias=False)\n",
        "        self.fc = nn.Linear(d_v, d_model, bias=False)\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k**0.5)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "        residual = q\n",
        "        \n",
        "        q = self.w_qs(q).view(sz_b, len_q, 1, self.d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, 1, self.d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, 1, self.d_v)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "        q = self.layer_norm(q)\n",
        "        return q, attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_src_vocab, d_model, n_position, d_k=164, d_v=164, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_model, padding_idx=pad_idx)\n",
        "        self.position_enc = PositionalEncoding(d_model, n_position=n_position)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.self_attention = SelfAttention(d_model, d_k, d_v, dropout=dropout)\n",
        "    \n",
        "    def forward(self, src_seq, mask):\n",
        "        enc_output = self.src_word_emb(src_seq)\n",
        "        enc_output = self.position_enc(enc_output)\n",
        "        enc_output = self.dropout(enc_output)\n",
        "        enc_output = self.layer_norm(enc_output)\n",
        "        enc_output, _ = self.self_attention(enc_output, enc_output, enc_output, mask=mask)\n",
        "        return enc_output\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_sequences, max_sequence_length, num_actions, max_action_value, d_model=64):\n",
        "        super().__init__()\n",
        "        self.num_sequences = num_sequences\n",
        "        self.num_rows = num_sequences + 2\n",
        "        \n",
        "        dim = (num_sequences + 2) * max_sequence_length\n",
        "        self.encoder = Encoder(6, d_model, dim)\n",
        "        self.seq_embedding = nn.Embedding(self.num_rows, d_model)\n",
        "        nn.init.normal_(self.seq_embedding.weight, 0.0, 0.1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(dim * d_model, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_actions)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, R, C = x.shape\n",
        "        \n",
        "        x_flat = x.view(B, R * C)\n",
        "        mask_flat = (x_flat != 0).unsqueeze(1)\n",
        "        h = self.encoder(x_flat, mask_flat)\n",
        "        \n",
        "        if h.dim() == 3:\n",
        "            h = h.view(B, R, C, -1)\n",
        "        \n",
        "        row_ids = torch.arange(R, device=h.device)\n",
        "        row_emb = self.seq_embedding(row_ids)[None, :, None, :]\n",
        "        h = h + row_emb\n",
        "        \n",
        "        h = h.reshape(B, -1)\n",
        "        h = F.leaky_relu(self.fc1(h))\n",
        "        h = self.dropout(h)\n",
        "        h = F.leaky_relu(self.fc2(h))\n",
        "        h = self.dropout(h)\n",
        "        q = self.fc3(h)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:    \n",
        "    def __init__(self, action_number, num_seqs, max_grid, max_value,\n",
        "                 epsilon=0.8, delta=0.05, decrement_iteration=5,\n",
        "                 update_iteration=128, batch_size=128, gamma=1.0,\n",
        "                 learning_rate=0.001, memory_size=1000):\n",
        "        self.action_number = action_number\n",
        "        self.num_seqs = num_seqs\n",
        "        self.max_grid = max_grid\n",
        "        self.seq_num = num_seqs\n",
        "        self.max_seq_len = max_grid + 1\n",
        "        \n",
        "        self.eval_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        \n",
        "        self.replay_memory = ReplayMemory(memory_size=memory_size)\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=learning_rate)\n",
        "        self.loss_func = nn.SmoothL1Loss()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.update_iteration = update_iteration\n",
        "        self.update_step_counter = 0\n",
        "        self.tau = 0.005\n",
        "        self.use_double_dqn = True\n",
        "        \n",
        "        self.initial_epsilon = epsilon\n",
        "        self.current_epsilon = epsilon\n",
        "        self.epsilon_end = delta\n",
        "        self.epsilon_decay = 0.999\n",
        "        \n",
        "        self.losses = []\n",
        "        self.epsilons = []\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.update_step_counter < 5000:\n",
        "            decay_rate = 0.9999\n",
        "        elif self.update_step_counter < 10000:\n",
        "            decay_rate = 0.999\n",
        "        else:\n",
        "            decay_rate = 0.995\n",
        "        \n",
        "        self.current_epsilon = max(self.epsilon_end, self.current_epsilon * decay_rate)\n",
        "        self.epsilons.append(self.current_epsilon)\n",
        "\n",
        "    def select_action(self, state, valid_action_mask=None):\n",
        "        is_random = (random.random() <= self.current_epsilon)\n",
        "        \n",
        "        if is_random:\n",
        "            if valid_action_mask is not None:\n",
        "                valid_idx = np.flatnonzero(valid_action_mask)\n",
        "                action = int(np.random.choice(valid_idx)) if len(valid_idx) else random.randrange(self.action_number)\n",
        "            else:\n",
        "                action = random.randrange(self.action_number)\n",
        "        else:\n",
        "            self.eval_net.eval()\n",
        "            with torch.no_grad():\n",
        "                s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num + 2, self.max_seq_len)\n",
        "                q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "            self.eval_net.train()\n",
        "            \n",
        "            if valid_action_mask is not None:\n",
        "                q[~valid_action_mask] = -np.inf\n",
        "            \n",
        "            action = int(np.argmax(q))\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def predict(self, state, valid_action_mask=None):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num + 2, self.max_seq_len)\n",
        "            q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "        self.eval_net.train()\n",
        "        \n",
        "        if valid_action_mask is not None:\n",
        "            q[~valid_action_mask] = -np.inf\n",
        "        \n",
        "        return int(np.nanargmax(q))\n",
        "\n",
        "    def forward(self, state):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(\n",
        "                1, self.seq_num + 2, self.max_seq_len\n",
        "            )\n",
        "            q = self.eval_net(s).squeeze(0)\n",
        "        self.eval_net.train()\n",
        "        return q\n",
        "    \n",
        "    @property\n",
        "    def epsilon(self):\n",
        "        return self.current_epsilon\n",
        "\n",
        "    def update(self):\n",
        "        self.update_step_counter += 1\n",
        "        \n",
        "        if self.replay_memory.size < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        state, next_state, action, reward, done, next_mask = self.replay_memory.sample(self.batch_size)\n",
        "        \n",
        "        batch_state = torch.LongTensor(state).to(device).view(-1, self.seq_num + 2, self.max_seq_len)\n",
        "        batch_next_state = torch.LongTensor(next_state).to(device).view(-1, self.seq_num + 2, self.max_seq_len)\n",
        "        batch_action = torch.LongTensor(action).unsqueeze(-1).to(device)\n",
        "        batch_reward = torch.FloatTensor(reward).unsqueeze(-1).to(device)\n",
        "        batch_done = torch.FloatTensor(done).unsqueeze(-1).to(device)\n",
        "        batch_next_mask = torch.BoolTensor(next_mask).to(device)\n",
        "        \n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if self.use_double_dqn:\n",
        "                q_next_online = self.eval_net(batch_next_state)\n",
        "                q_next_online_masked = q_next_online.clone()\n",
        "                q_next_online_masked[~batch_next_mask] = float('-inf')\n",
        "                best_next_actions = q_next_online_masked.max(1)[1]\n",
        "                \n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next = q_next_target.gather(1, best_next_actions.unsqueeze(1))\n",
        "            else:\n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next_masked = q_next_target.clone()\n",
        "                q_next_masked[~batch_next_mask] = float('-inf')\n",
        "                q_next = q_next_masked.max(1)[0].unsqueeze(-1)\n",
        "            \n",
        "            q_next = torch.where(torch.isinf(q_next), torch.zeros_like(q_next), q_next)\n",
        "            q_target = batch_reward + (1.0 - batch_done) * self.gamma * q_next\n",
        "            q_target = torch.clamp(q_target, -10.0, 10.0)\n",
        "        \n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.eval_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for target_param, online_param in zip(self.target_net.parameters(), self.eval_net.parameters()):\n",
        "                target_param.data.mul_(1.0 - self.tau)\n",
        "                target_param.data.add_(self.tau * online_param.data)\n",
        "        \n",
        "        self.losses.append(loss.item())\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlignmentEnvironment:\n",
        "    def __init__(self, sequences, consensus, total_gap, max_n_seqs=None, max_cons_len=None):\n",
        "        self.consensus = consensus\n",
        "        self.original_n_seqs = len(sequences)\n",
        "        self.original_cons_len = len(consensus)\n",
        "        \n",
        "        self.max_n_seqs = max_n_seqs if max_n_seqs is not None else len(sequences)\n",
        "        self.max_cons_len = max_cons_len if max_cons_len is not None else len(consensus)\n",
        "        \n",
        "        padded_consensus = consensus + [('PAD', 'PAD')] * (self.max_cons_len - len(consensus))\n",
        "        sequences = self._pad_sequences(sequences, self.max_cons_len)\n",
        "        while len(sequences) < self.max_n_seqs:\n",
        "            sequences.append('-' * self.max_cons_len)\n",
        "        \n",
        "        self.sequences = sequences\n",
        "        self.initial_sequences = copy.deepcopy(sequences)\n",
        "        self.sep_nuc_in_seq = [[letter for letter in seq] for seq in sequences]\n",
        "        self.label_encoded_seqs = [[NUCLEOTIDE_MAP[char] for char in seq] for seq in sequences]\n",
        "        self.consensus_encoded = self._encode_consensus(padded_consensus)\n",
        "        \n",
        "        self.num_seqs = self.max_n_seqs\n",
        "        self.action_number = self.max_n_seqs * self.max_cons_len\n",
        "        self.aligned = [[] for _ in range(self.num_seqs)]\n",
        "        self.original = copy.deepcopy(self.label_encoded_seqs)\n",
        "        \n",
        "        self.original_score = self._gearbox_score(consensus, self.sep_nuc_in_seq[:self.original_n_seqs])\n",
        "        self.last_score = self.original_score\n",
        "        \n",
        "        self.total_gap = total_gap\n",
        "        self.initial_gap = total_gap\n",
        "        self.gaps_per_sequence = [0] * self.num_seqs\n",
        "        \n",
        "        self.recent_actions = []\n",
        "        self.last_seq_chosen = None\n",
        "        self.seq_streak_count = 0\n",
        "        \n",
        "        self.seq_permutation = None\n",
        "        self.seq_permutation_inv = None\n",
        "    \n",
        "    @staticmethod\n",
        "    def _pad_sequences(sequences, target_len):\n",
        "        return [s.ljust(target_len, '-') for s in sequences]\n",
        "    \n",
        "    def _encode_consensus(self, consensus):\n",
        "        cons_1 = [NUCLEOTIDE_MAP.get(p[0], 0) for p in consensus]\n",
        "        cons_2 = [NUCLEOTIDE_MAP.get(p[1], 0) for p in consensus]\n",
        "        return [cons_1, cons_2]\n",
        "    \n",
        "    def get_valid_action_mask(self):\n",
        "        A = self.action_number\n",
        "        mask = np.zeros(A, dtype=bool)\n",
        "        for seq_idx in range(self.original_n_seqs):\n",
        "            row = self.sequences[seq_idx]\n",
        "            for pos in range(self.max_cons_len):\n",
        "                action = seq_idx * self.max_cons_len + pos\n",
        "                mask[action] = (row[pos] != '-')\n",
        "        return mask\n",
        "    \n",
        "    @staticmethod\n",
        "    def _gearbox_score(consensus, sequences):\n",
        "        '''Calculate Column score (how many bases match with the reference) with bonus for perfect columns'''\n",
        "        bonus = 1.15\n",
        "        score = 0\n",
        "        max_col = min(len(consensus), min(len(seq) for seq in sequences) if sequences else 0)\n",
        "        for col_ind in range(max_col):\n",
        "            col_bonus = True\n",
        "            col_tot = 0\n",
        "            for row in sequences:\n",
        "                if col_ind >= len(row):\n",
        "                    continue\n",
        "                char = row[col_ind]\n",
        "                if char == '-':\n",
        "                    col_bonus = False\n",
        "                    continue\n",
        "                if col_ind < len(consensus) and char in consensus[col_ind]:\n",
        "                    col_tot += 1\n",
        "                else:\n",
        "                    col_bonus = False\n",
        "            if col_bonus:\n",
        "                score += col_tot * bonus\n",
        "            else:\n",
        "                score += col_tot\n",
        "        return score\n",
        "    \n",
        "    def get_current_state(self):\n",
        "        state = []\n",
        "        for seq in self.original:\n",
        "            state.extend(seq)\n",
        "        state.extend(self.consensus_encoded[0])\n",
        "        state.extend(self.consensus_encoded[1])\n",
        "        return state\n",
        "    \n",
        "    def calc_reward(self):\n",
        "        current_alignment = self.get_alignment()\n",
        "        rows = [[letter for letter in seq] for seq in current_alignment[:self.original_n_seqs]]\n",
        "        return self._gearbox_score(self.consensus[:self.original_cons_len], rows)\n",
        "    \n",
        "    def _count_perfect_columns(self):\n",
        "        current_alignment = self.get_alignment()\n",
        "        rows = [[letter for letter in seq] for seq in current_alignment[:self.original_n_seqs]]\n",
        "        perfect_count = 0\n",
        "        max_col = min(self.original_cons_len, min(len(r) for r in rows) if rows else 0)\n",
        "        for col_ind in range(max_col):\n",
        "            all_match = True\n",
        "            has_nucleotide = False\n",
        "            for row in rows:\n",
        "                if col_ind >= len(row):\n",
        "                    continue\n",
        "                char = row[col_ind]\n",
        "                if char == '-':\n",
        "                    continue\n",
        "                has_nucleotide = True\n",
        "                if col_ind < len(self.consensus) and char not in self.consensus[col_ind]:\n",
        "                    all_match = False\n",
        "                    break\n",
        "            if all_match and has_nucleotide:\n",
        "                perfect_count += 1\n",
        "        return perfect_count\n",
        "    \n",
        "    def _calculate_position_aware_reward(self, seq_idx, pos):\n",
        "        if seq_idx >= self.original_n_seqs or pos >= self.original_cons_len:\n",
        "            return 0.0\n",
        "        \n",
        "        sequence = self.original[seq_idx]\n",
        "        reward = 0\n",
        "        look_ahead = min(5, len(sequence) - pos - 1)\n",
        "        for offset in range(1, look_ahead + 1):\n",
        "            nuc_pos = pos + offset\n",
        "            if nuc_pos >= self.original_cons_len:\n",
        "                break\n",
        "            nuc_val = sequence[nuc_pos]\n",
        "            if nuc_val in [5, 0]:\n",
        "                continue\n",
        "            nuc = NUCLEOTIDES[nuc_val]\n",
        "            if nuc_pos < len(self.consensus) and nuc in self.consensus[nuc_pos]:\n",
        "                reward += 1.0 / offset\n",
        "        return reward\n",
        "    \n",
        "    def _calculate_diversity_bonus(self):\n",
        "        gaps_in_real_seqs = self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        total_gaps = sum(gaps_in_real_seqs)\n",
        "        \n",
        "        if total_gaps == 0 or self.original_n_seqs == 1:\n",
        "            return 0.0\n",
        "        \n",
        "        gaps_array = np.array(gaps_in_real_seqs, dtype=float)\n",
        "        n = self.original_n_seqs\n",
        "        sorted_gaps = np.sort(gaps_array)\n",
        "        \n",
        "        gini = (2.0 * np.sum((np.arange(n) + 1) * sorted_gaps)) / (n * total_gaps) - (n + 1.0) / n\n",
        "        diversity_score = 1.0 - 2.0 * gini\n",
        "        \n",
        "        return diversity_score\n",
        "    \n",
        "    def _calculate_concentration_penalty(self):\n",
        "        gaps_in_real_seqs = self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        total_gaps = sum(gaps_in_real_seqs)\n",
        "        \n",
        "        if total_gaps == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        max_gaps_in_any_seq = max(gaps_in_real_seqs)\n",
        "        avg_gaps = total_gaps / self.original_n_seqs\n",
        "        \n",
        "        if max_gaps_in_any_seq > avg_gaps * 2.5:\n",
        "            return -5.0\n",
        "        elif max_gaps_in_any_seq > avg_gaps * 2.0:\n",
        "            return -3.0\n",
        "        elif max_gaps_in_any_seq > avg_gaps * 1.5:\n",
        "            return -1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    def _diversity_entropy(self, scale: float = 20.0) -> float:\n",
        "        gaps = self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        S = float(sum(gaps))\n",
        "        if S <= 0.0:\n",
        "            return 0.0\n",
        "        \n",
        "        p = np.asarray(gaps, dtype=np.float64) / S\n",
        "        nz = p > 0\n",
        "        R_eff = int(nz.sum())\n",
        "        if R_eff <= 1:\n",
        "            return 0.0\n",
        "        \n",
        "        H = float(-(p[nz] * np.log(p[nz] + 1e-12)).sum())\n",
        "        H_norm = H / np.log(float(R_eff))\n",
        "        return scale * H_norm\n",
        "    \n",
        "    def step(self, action):\n",
        "        seq_idx = int(action) // self.max_cons_len\n",
        "        pos = int(action) % self.max_cons_len\n",
        "        \n",
        "        if not (0 <= seq_idx < self.original_n_seqs and 0 <= pos < self.max_cons_len):\n",
        "            reward = -1.0\n",
        "            return reward, self.get_current_state(), 0\n",
        "        \n",
        "        row = list(self.sequences[seq_idx])\n",
        "        placed = False\n",
        "        if row[pos] != '-':\n",
        "            row.insert(pos, '-')\n",
        "            row.pop()\n",
        "            self.sequences[seq_idx] = \"\".join(row)\n",
        "            self.sep_nuc_in_seq[seq_idx] = row\n",
        "            self.label_encoded_seqs[seq_idx] = [NUCLEOTIDE_MAP[char] for char in row]\n",
        "            self.original[seq_idx] = [NUCLEOTIDE_MAP[char] for char in row]\n",
        "            self.gaps_per_sequence[seq_idx] += 1\n",
        "            self.total_gap = max(0, self.total_gap - 1)\n",
        "            placed = True\n",
        "        \n",
        "        reward = 0.0\n",
        "        if placed:\n",
        "            reward += 0.5\n",
        "        else:\n",
        "            reward -= 0.1\n",
        "        \n",
        "        done = 1 if self.total_gap == 0 else 0\n",
        "        if done == 1:\n",
        "            reward += self._diversity_entropy(scale=20.0)\n",
        "        \n",
        "        reward = float(np.clip(reward, -25.0, 25.0))\n",
        "        \n",
        "        return reward, self.get_current_state(), done\n",
        "    \n",
        "    def get_alignment(self):\n",
        "        alignment = []\n",
        "        for i in range(len(self.aligned)):\n",
        "            alignment.append(''.join([NUCLEOTIDES[self.aligned[i][j]] for j in range(len(self.aligned[i]))]))\n",
        "        return alignment\n",
        "    \n",
        "    def get_original_alignment(self):\n",
        "        full_alignment = self.get_alignment()\n",
        "        return [full_alignment[i][:self.original_cons_len] for i in range(self.original_n_seqs)]\n",
        "    \n",
        "    def reset(self):\n",
        "        self.aligned = [[] for _ in range(self.num_seqs)]\n",
        "        self.original = copy.deepcopy(self.label_encoded_seqs)\n",
        "        self.total_gap = self.initial_gap\n",
        "        self.last_score = self.original_score\n",
        "        self.gaps_per_sequence = [0] * self.num_seqs\n",
        "        self.sequences = copy.deepcopy(self.initial_sequences)\n",
        "        self.sep_nuc_in_seq = [[letter for letter in seq] for seq in self.sequences]\n",
        "        self.recent_actions = []\n",
        "        self.last_seq_chosen = None\n",
        "        self.seq_streak_count = 0\n",
        "        self.seq_permutation = None\n",
        "        self.seq_permutation_inv = None\n",
        "        return self.get_current_state()\n",
        "    \n",
        "    def randomize_sequence_order(self, apply=True):\n",
        "        if not apply or self.original_n_seqs <= 1:\n",
        "            self.seq_permutation = None\n",
        "            self.seq_permutation_inv = None\n",
        "            return\n",
        "        \n",
        "        self.seq_permutation = np.random.permutation(self.original_n_seqs)\n",
        "        self.seq_permutation_inv = np.argsort(self.seq_permutation)\n",
        "        self.original[:self.original_n_seqs] = [self.original[i] for i in self.seq_permutation]\n",
        "        self.sequences[:self.original_n_seqs] = [self.sequences[i] for i in self.seq_permutation]\n",
        "        self.sep_nuc_in_seq[:self.original_n_seqs] = [self.sep_nuc_in_seq[i] for i in self.seq_permutation]\n",
        "        self.label_encoded_seqs[:self.original_n_seqs] = [self.label_encoded_seqs[i] for i in self.seq_permutation]\n",
        "        \n",
        "        gaps_copy = self.gaps_per_sequence[:self.original_n_seqs].copy()\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            self.gaps_per_sequence[new_idx] = gaps_copy[old_idx]\n",
        "    \n",
        "    def get_unpermuted_gaps(self):\n",
        "        if self.seq_permutation_inv is None:\n",
        "            return self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        \n",
        "        original_order = [0] * self.original_n_seqs\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            original_order[old_idx] = self.gaps_per_sequence[new_idx]\n",
        "        return original_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_prediction_episode(env, agent, expected_gaps, max_factor=1.25):\n",
        "    import math\n",
        "    import numpy as np\n",
        "    \n",
        "    state = env.reset()\n",
        "    max_steps = max(1, int(math.ceil(expected_gaps * max_factor)))\n",
        "    steps, rewards = 0, []\n",
        "    \n",
        "    for t in range(max_steps):\n",
        "        valid_mask = env.get_valid_action_mask().astype(bool)\n",
        "        \n",
        "        q_values = agent.forward(state)\n",
        "        action = masked_epsilon_greedy(\n",
        "            q_values=q_values,\n",
        "            valid_mask=valid_mask,\n",
        "            epsilon=getattr(agent, \"epsilon\", 0.1),\n",
        "        )\n",
        "        \n",
        "        reward, next_state, done = env.step(action)\n",
        "        rewards.append(float(reward))\n",
        "        \n",
        "        if hasattr(agent, \"replay_memory\"):\n",
        "            agent.replay_memory.store(\n",
        "                (state, next_state, action, reward, bool(done), valid_mask.copy())\n",
        "            )\n",
        "        \n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        \n",
        "        if done == 1 or not valid_mask.any():\n",
        "            break\n",
        "    \n",
        "    gaps_dist = env.get_unpermuted_gaps() if hasattr(env, \"get_unpermuted_gaps\") \\\n",
        "                else env.gaps_per_sequence[:env.original_n_seqs]\n",
        "    \n",
        "    return {\n",
        "        \"steps\": steps,\n",
        "        \"avg_reward\": float(np.mean(rewards)) if rewards else 0.0,\n",
        "        \"sum_reward\": float(np.sum(rewards)) if rewards else 0.0,\n",
        "        \"gaps_distribution\": gaps_dist,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    project_root\n",
        "except NameError:\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "def _parse_accepted_pairs(accepted_pairs_raw):\n",
        "    pairs = []\n",
        "    if isinstance(accepted_pairs_raw, str):\n",
        "        for pair in accepted_pairs_raw.split(','):\n",
        "            if ':' in pair:\n",
        "                a, b = pair.split(':', 1)\n",
        "                pairs.append((a.strip(), b.strip()))\n",
        "    elif isinstance(accepted_pairs_raw, list):\n",
        "        for pair in accepted_pairs_raw:\n",
        "            if isinstance(pair, (list, tuple)) and len(pair) == 2:\n",
        "                pairs.append((str(pair[0]), str(pair[1])))\n",
        "    return pairs\n",
        "\n",
        "def _count_inserted_gaps_from_sequences(start, solution):\n",
        "    dash_start = sum(str(s).count('-') for s in start)\n",
        "    dash_solution = sum(str(s).count('-') for s in solution)\n",
        "    return max(0, dash_solution - dash_start)\n",
        "\n",
        "def load_samples_from_jsonl(jsonl_path='hackaton_extracted_data.jsonl', max_samples=None, shuffle_data=True):\n",
        "    if not os.path.isabs(jsonl_path):\n",
        "        jsonl_path = project_root / jsonl_path\n",
        "\n",
        "    if not os.path.exists(jsonl_path):\n",
        "        return []\n",
        "\n",
        "    samples = []\n",
        "    \n",
        "    with open(jsonl_path, 'r') as f:\n",
        "        for line_num, line in enumerate(f):\n",
        "            if max_samples and len(samples) >= max_samples:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                \n",
        "                start = data.get('start', [])\n",
        "                solution = data.get('solution', [])\n",
        "                accepted_pairs_raw = data.get('accepted_pairs', [])\n",
        "                moves = data.get('moves', [])\n",
        "                \n",
        "                accepted_pairs = _parse_accepted_pairs(accepted_pairs_raw)\n",
        "                \n",
        "                if isinstance(moves, list):\n",
        "                    n_gaps = len(moves)\n",
        "                elif isinstance(moves, int):\n",
        "                    n_gaps = moves\n",
        "                else:\n",
        "                    n_gaps = _count_inserted_gaps_from_sequences(start, solution)\n",
        "                \n",
        "                n_gaps = max(0, int(n_gaps))\n",
        "                \n",
        "                if start and solution and accepted_pairs and len(start) == len(solution):\n",
        "                    samples.append({\n",
        "                        'start': start,\n",
        "                        'solution': solution,\n",
        "                        'accepted_pairs': accepted_pairs,\n",
        "                        'n_gaps': n_gaps,\n",
        "                        'moves': moves,\n",
        "                        'n_sequences': len(start),\n",
        "                        'consensus_length': len(accepted_pairs),\n",
        "                        'idx': line_num,\n",
        "                    })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    if shuffle_data and samples:\n",
        "        random.shuffle(samples)\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Dimension Analysis (Using ALL Data with Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n_seqs = max(s['n_sequences'] for s in samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in samples)\n",
        "\n",
        "for sample in samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training with Variable Dimensions (Using Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(samples))\n",
        "train_samples = samples[:train_size]\n",
        "test_samples = samples[train_size:]\n",
        "\n",
        "action_number = max_n_seqs * max_cons_len\n",
        "agent = DQNAgent(\n",
        "    action_number=action_number,\n",
        "    num_seqs=max_n_seqs,\n",
        "    max_grid=max_cons_len - 1,\n",
        "    max_value=(max_cons_len - 1) * 100,\n",
        "    epsilon=0.9,\n",
        "    delta=0.01,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    learning_rate=0.001,\n",
        "    memory_size=5000\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "samples_per_epoch = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_samples = random.sample(train_samples, min(len(train_samples), samples_per_epoch))\n",
        "    epoch_rewards = []\n",
        "    \n",
        "    for sample in tqdm(epoch_samples, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        env = AlignmentEnvironment(\n",
        "            sample['start'],\n",
        "            sample['accepted_pairs'],\n",
        "            get_expected_gaps(sample),\n",
        "            max_n_seqs=max_n_seqs,\n",
        "            max_cons_len=max_cons_len\n",
        "        )\n",
        "        \n",
        "        state = env.reset()\n",
        "        env.randomize_sequence_order(apply=True)\n",
        "        \n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "        max_steps = get_expected_gaps(sample) * 5\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "        \n",
        "        while step_count < max_steps:\n",
        "            action = agent.select_action(state, valid_action_mask=valid_mask)\n",
        "            reward, next_state, done = env.step(action)\n",
        "            \n",
        "            next_valid_mask = env.get_valid_action_mask()\n",
        "            agent.replay_memory.store((state, next_state, action, reward, done, next_valid_mask))\n",
        "            \n",
        "            agent.update()\n",
        "            \n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "            state = next_state\n",
        "            valid_mask = next_valid_mask\n",
        "            \n",
        "            if done == 1:\n",
        "                break\n",
        "        \n",
        "        agent.update_epsilon()\n",
        "        epoch_rewards.append(episode_reward)\n",
        "    \n",
        "    avg_reward = np.mean(epoch_rewards)\n",
        "    print(f\"Epoch {epoch+1}: reward={avg_reward:.2f}, memory={agent.replay_memory.size}, Îµ={agent.current_epsilon:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_dqn_inference(agent, env, expected_gaps, max_steps=None):\n",
        "    if max_steps is None:\n",
        "        max_steps = expected_gaps * 2\n",
        "    \n",
        "    state = env.reset()\n",
        "    env.randomize_sequence_order(apply=True)\n",
        "    \n",
        "    actions_taken = []\n",
        "    valid_mask = env.get_valid_action_mask()\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        if valid_mask.sum() == 0:\n",
        "            break\n",
        "        \n",
        "        action = agent.predict(state, valid_action_mask=valid_mask)\n",
        "        \n",
        "        seq_idx = action // env.max_cons_len\n",
        "        pos = action % env.max_cons_len\n",
        "        actions_taken.append((seq_idx, pos))\n",
        "        \n",
        "        _, next_state, done = env.step(action)\n",
        "        state = next_state\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "        \n",
        "        if done == 1:\n",
        "            break\n",
        "    \n",
        "    predicted = env.sequences[:env.original_n_seqs]\n",
        "    \n",
        "    return predicted, actions_taken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "COMP545",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
