{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The device being used is: cpu\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "# Setup\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"The device being used is: {device}\")\n",
        "\n",
        "# Constants\n",
        "NUCLEOTIDE_MAP = {\"P\": 0, \"A\": 1, \"T\": 2, \"C\": 3, \"G\": 4}\n",
        "NUCLEOTIDES = {v: k for k, v in NUCLEOTIDE_MAP.items()}\n",
        "GAP_CHAR = '-'  # for internal use during column construction\n",
        "\n",
        "# config file info\n",
        "training_size = 1000\n",
        "max_nt_num = 150\n",
        "MAX_MSA_LEN = 50\n",
        "MAX_N_SEQS = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def masked_epsilon_greedy(q_values: torch.Tensor, valid_mask: np.ndarray, epsilon: float, rng=None) -> int:\\n    \"\"\"Epsilon-greedy action selection respecting valid action mask.\"\"\"\\n    q_values = q_values.flatten()\\n    valid_mask = np.asarray(valid_mask, dtype=bool)\\n    rng = rng or np.random\\n\\n    valid_idx = np.flatnonzero(valid_mask)\\n    if valid_idx.size == 0:\\n        return int(torch.argmax(q_values).item())\\n\\n    # Exploration\\n    if rng.random() < epsilon:\\n        return int(rng.choice(valid_idx))\\n\\n    # Exploitation\\n    q = q_values.detach().cpu().numpy()\\n    q[~valid_mask] = -np.inf\\n    return int(np.argmax(q))\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def masked_epsilon_greedy(q_values: torch.Tensor, valid_mask: np.ndarray, epsilon: float, rng=None) -> int:\n",
        "    if q_values.ndim > 1:\n",
        "        q_values = q_values.reshape(-1)\n",
        "    valid_mask = np.asarray(valid_mask, dtype=bool)\n",
        "    \n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    \n",
        "    valid_idx = np.flatnonzero(valid_mask)\n",
        "    \n",
        "    if valid_idx.size == 0:\n",
        "        return int(torch.argmax(q_values).item())\n",
        "    \n",
        "    if rng.random() < epsilon:\n",
        "        return int(rng.choice(valid_idx))\n",
        "    \n",
        "    q = q_values.detach().cpu().numpy().copy()\n",
        "    q[~valid_mask] = -np.inf\n",
        "    return int(np.argmax(q))\n",
        "\n",
        "'''def masked_epsilon_greedy(q_values: torch.Tensor, valid_mask: np.ndarray, epsilon: float, rng=None) -> int:\n",
        "    \"\"\"Epsilon-greedy action selection respecting valid action mask.\"\"\"\n",
        "    q_values = q_values.flatten()\n",
        "    valid_mask = np.asarray(valid_mask, dtype=bool)\n",
        "    rng = rng or np.random\n",
        "\n",
        "    valid_idx = np.flatnonzero(valid_mask)\n",
        "    if valid_idx.size == 0:\n",
        "        return int(torch.argmax(q_values).item())\n",
        "\n",
        "    # Exploration\n",
        "    if rng.random() < epsilon:\n",
        "        return int(rng.choice(valid_idx))\n",
        "\n",
        "    # Exploitation\n",
        "    q = q_values.detach().cpu().numpy()\n",
        "    q[~valid_mask] = -np.inf\n",
        "    return int(np.argmax(q))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_expected_alignment(sample):\n",
        "    for key in (\"solution\", \"aligned\", \"target\"):\n",
        "        if key in sample and sample[key] is not None:\n",
        "            result = sample[key]\n",
        "            if isinstance(result, str):\n",
        "                return [result]\n",
        "            return [str(s) for s in result]\n",
        "    return None\n",
        "\n",
        "def get_expected_steps(sample, scale_factor=1.2, min_steps=10, max_steps=300):\n",
        "    \"\"\"\n",
        "    Estimate expected number of alignment steps (columns) for the column-based environment.\n",
        "    This approximates the final alignment length, without looking at the ground truth.\n",
        "    \"\"\"\n",
        "    if 'start' in sample and isinstance(sample['start'], list):\n",
        "        lengths = [len(seq) for seq in sample['start']]\n",
        "        avg_len = np.mean(lengths)\n",
        "        max_len = max(lengths)\n",
        "\n",
        "        # Estimate final aligned length (a few gaps will expand it)\n",
        "        est_len = int(scale_factor * max_len)\n",
        "        return int(np.clip(est_len, min_steps, max_steps))\n",
        "\n",
        "    # Fallback default because max length of sequences is 50 \n",
        "    return 50\n",
        "\n",
        "def get_expected_gaps(sample, max_factor=2.0, min_steps=1, max_steps_cap=1000):\n",
        "    \"\"\"\n",
        "    Column-based budget: at most ~2 gaps per original column.\n",
        "    No GT usage; only looks at raw starting sequences.\n",
        "    \"\"\"\n",
        "    seqs = sample[\"start\"]\n",
        "    # print(f\"withint get expected gaps, seqs: {seqs}\")\n",
        "    L = max(len(s) for s in seqs)\n",
        "    steps = int(max_factor * L)          # e.g., 2 * longest raw length\n",
        "    return max(min_steps, min(steps, max_steps_cap))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, memory_size=1000):\n",
        "        self.storage = []\n",
        "        self.memory_size = memory_size\n",
        "        self.size = 0\n",
        "    \n",
        "    def store(self, data: tuple):\n",
        "        if len(self.storage) == self.memory_size:\n",
        "            self.storage.pop(0)\n",
        "        self.storage.append(data)\n",
        "        self.size = min(self.size + 1, self.memory_size)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.storage, batch_size)\n",
        "        state = [s for s, _, _, _, _, _ in samples]\n",
        "        next_state = [ns for _, ns, _, _, _, _ in samples]\n",
        "        action = [a for _, _, a, _, _, _ in samples]\n",
        "        reward = [r for _, _, _, r, _, _ in samples]\n",
        "        done = [d for _, _, _, _, d, _ in samples]\n",
        "        next_mask = [m for _, _, _, _, _, m in samples]\n",
        "        return state, next_state, action, reward, done, next_mask\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "        return output, attn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_hid, n_position=200):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"pos_table\", self._get_sinusoid_table(n_position, d_hid))\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_sinusoid_table(n_position, d_hid):\n",
        "        positions = torch.arange(n_position).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_hid, 2).float() * (-np.log(10000.0) / d_hid))\n",
        "        sin = torch.sin(positions * div_term)\n",
        "        cos = torch.cos(positions * div_term)\n",
        "        return torch.cat([sin, cos], dim=-1).unsqueeze(0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pos_table[:, :x.size(1), :].clone().detach()\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.w_qs = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, d_v, bias=False)\n",
        "        self.fc = nn.Linear(d_v, d_model, bias=False)\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k**0.5)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "        residual = q\n",
        "        \n",
        "        q = self.w_qs(q).view(sz_b, len_q, 1, self.d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, 1, self.d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, 1, self.d_v)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "        q = self.layer_norm(q)\n",
        "        return q, attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_src_vocab, d_model, n_position, d_k=164, d_v=164, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_model, padding_idx=pad_idx)\n",
        "        self.position_enc = PositionalEncoding(d_model, n_position=n_position)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.self_attention = SelfAttention(d_model, d_k, d_v, dropout=dropout)\n",
        "    \n",
        "    def forward(self, src_seq, mask):\n",
        "        enc_output = self.src_word_emb(src_seq)\n",
        "        enc_output = self.position_enc(enc_output)\n",
        "        enc_output = self.dropout(enc_output)\n",
        "        enc_output = self.layer_norm(enc_output)\n",
        "        enc_output, _ = self.self_attention(enc_output, enc_output, enc_output, mask=mask)\n",
        "        return enc_output\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_sequences, max_sequence_length, num_actions, max_action_value, d_model=64):\n",
        "        super().__init__()\n",
        "        self.num_sequences = num_sequences\n",
        "        self.num_rows = num_sequences  # <-- no +2\n",
        "\n",
        "        dim = self.num_rows * max_sequence_length\n",
        "        # Encoder: your existing module that maps (B, dim) with an attention/MLP stack\n",
        "        self.encoder = Encoder(5, d_model, dim)                                                             ####### Changed from 6 to 5 to get rid of gap since column based will never encounter gaps in the embedding stage\n",
        "\n",
        "        # Learnable row embeddings (still helpful)\n",
        "        self.seq_embedding = nn.Embedding(self.num_rows, d_model)\n",
        "        nn.init.normal_(self.seq_embedding.weight, 0.0, 0.1)\n",
        "\n",
        "        self.fc1 = nn.Linear(dim * d_model, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_actions)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, R, C = x.shape          # R == num_sequences\n",
        "        x_flat = x.view(B, R * C)\n",
        "        mask_flat = (x_flat != 0).unsqueeze(1)\n",
        "\n",
        "        h = self.encoder(x_flat, mask_flat)   # shape can be (B, R*C, d_model) or (B, d_model) per your Encoder\n",
        "        if h.dim() == 3:\n",
        "            h = h.view(B, R, C, -1)\n",
        "\n",
        "        # add per-row embedding (broadcast)\n",
        "        row_ids = torch.arange(R, device=h.device)\n",
        "        row_emb = self.seq_embedding(row_ids)[None, :, None, :]  # (1,R,1,d_model)\n",
        "        h = h + row_emb\n",
        "\n",
        "        h = h.reshape(B, -1)\n",
        "        h = F.leaky_relu(self.fc1(h)); h = self.dropout(h)\n",
        "        h = F.leaky_relu(self.fc2(h)); h = self.dropout(h)\n",
        "        q = self.fc3(h)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:    \n",
        "    def __init__(self, action_number, num_seqs, max_grid, max_value,\n",
        "                 epsilon=0.8, delta=0.05, decrement_iteration=5,\n",
        "                 update_iteration=128, batch_size=128, gamma=1.0,\n",
        "                 learning_rate=0.001, memory_size=1000):\n",
        "        self.seq_num = num_seqs\n",
        "        self.max_seq_len = max_grid  # <-- not +1\n",
        "        self.action_number = action_number\n",
        "\n",
        "        self.eval_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net = QNetwork(num_seqs, self.max_seq_len, action_number, max_value).to(device)\n",
        "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        \n",
        "        self.replay_memory = ReplayMemory(memory_size=memory_size)\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=learning_rate)\n",
        "        self.loss_func = nn.SmoothL1Loss()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.update_iteration = update_iteration\n",
        "        self.update_step_counter = 0\n",
        "        self.tau = 0.005\n",
        "        self.use_double_dqn = True\n",
        "        \n",
        "        self.initial_epsilon = epsilon\n",
        "        self.current_epsilon = epsilon\n",
        "        self.epsilon_end = delta\n",
        "        self.epsilon_decay = 0.999\n",
        "        \n",
        "        self.losses = []\n",
        "        self.epsilons = []\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.update_step_counter < 5000:\n",
        "            decay_rate = 0.9999\n",
        "        elif self.update_step_counter < 10000:\n",
        "            decay_rate = 0.999\n",
        "        else:\n",
        "            decay_rate = 0.995\n",
        "        \n",
        "        self.current_epsilon = max(self.epsilon_end, self.current_epsilon * decay_rate)\n",
        "        self.epsilons.append(self.current_epsilon)\n",
        "\n",
        "    def select_action(self, state, valid_action_mask=None):\n",
        "        is_random = (random.random() <= self.current_epsilon)\n",
        "        \n",
        "        if is_random:\n",
        "            if valid_action_mask is not None:\n",
        "                valid_idx = np.flatnonzero(valid_action_mask)\n",
        "                action = int(np.random.choice(valid_idx)) if len(valid_idx) else random.randrange(self.action_number)\n",
        "            else:\n",
        "                action = random.randrange(self.action_number)\n",
        "        else:\n",
        "            self.eval_net.eval()\n",
        "            with torch.no_grad():\n",
        "                s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num, self.max_seq_len)\n",
        "                q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "            self.eval_net.train()\n",
        "            \n",
        "            if valid_action_mask is not None:\n",
        "                q[~valid_action_mask] = -np.inf\n",
        "            \n",
        "            action = int(np.argmax(q))\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def predict(self, state, valid_action_mask=None):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(1, self.seq_num, self.max_seq_len)\n",
        "            q = self.eval_net(s).squeeze(0).detach().cpu().numpy()\n",
        "        self.eval_net.train()\n",
        "        \n",
        "        if valid_action_mask is not None:\n",
        "            q[~valid_action_mask] = -np.inf\n",
        "        \n",
        "        return int(np.nanargmax(q))\n",
        "\n",
        "    def forward(self, state):\n",
        "        self.eval_net.eval()\n",
        "        with torch.no_grad():\n",
        "            s = torch.as_tensor(state, dtype=torch.long, device=device).view(\n",
        "                1, self.seq_num, self.max_seq_len\n",
        "            )\n",
        "            q = self.eval_net(s).squeeze(0)\n",
        "        self.eval_net.train()\n",
        "        return q\n",
        "    \n",
        "    @property\n",
        "    def epsilon(self):\n",
        "        return self.current_epsilon\n",
        "\n",
        "    def update(self):\n",
        "        self.update_step_counter += 1\n",
        "        \n",
        "        if self.replay_memory.size < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        state, next_state, action, reward, done, next_mask = self.replay_memory.sample(self.batch_size)\n",
        "\n",
        "        batch_state = torch.LongTensor(state).to(device).view(-1, self.seq_num, self.max_seq_len)\n",
        "        batch_next_state = torch.LongTensor(next_state).to(device).view(-1, self.seq_num, self.max_seq_len)\n",
        "        batch_action = torch.LongTensor(action).unsqueeze(-1).to(device)\n",
        "        batch_reward = torch.FloatTensor(reward).unsqueeze(-1).to(device)\n",
        "        batch_done = torch.FloatTensor(done).unsqueeze(-1).to(device)\n",
        "        batch_next_mask = torch.BoolTensor(next_mask).to(device)\n",
        "        \n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if self.use_double_dqn:\n",
        "                q_next_online = self.eval_net(batch_next_state)\n",
        "                q_next_online_masked = q_next_online.clone()\n",
        "                q_next_online_masked[~batch_next_mask] = float('-inf')\n",
        "                best_next_actions = q_next_online_masked.max(1)[1]\n",
        "                \n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next = q_next_target.gather(1, best_next_actions.unsqueeze(1))\n",
        "            else:\n",
        "                q_next_target = self.target_net(batch_next_state)\n",
        "                q_next_masked = q_next_target.clone()\n",
        "                q_next_masked[~batch_next_mask] = float('-inf')\n",
        "                q_next = q_next_masked.max(1)[0].unsqueeze(-1)\n",
        "            \n",
        "            q_next = torch.where(torch.isinf(q_next), torch.zeros_like(q_next), q_next)\n",
        "            q_target = batch_reward + (1.0 - batch_done) * self.gamma * q_next\n",
        "            q_target = torch.clamp(q_target, -10.0, 10.0)\n",
        "        \n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.eval_net.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for target_param, online_param in zip(self.target_net.parameters(), self.eval_net.parameters()):\n",
        "                target_param.data.mul_(1.0 - self.tau)\n",
        "                target_param.data.add_(self.tau * online_param.data)\n",
        "        \n",
        "        self.losses.append(loss.item())\n",
        "        return loss.item()\n",
        "    \n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save the current model (eval_net) and target_net weights.\"\"\"\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save({\n",
        "            'eval_net_state_dict': self.eval_net.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.current_epsilon,\n",
        "            'update_step_counter': self.update_step_counter\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path, map_location=None):\n",
        "        \"\"\"Load model weights from a saved checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=map_location or device)\n",
        "        self.eval_net.load_state_dict(checkpoint['eval_net_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.current_epsilon = checkpoint.get('epsilon', self.current_epsilon)\n",
        "        self.update_step_counter = checkpoint.get('update_step_counter', 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example NUCLEOTIDE_MAP / NUCLEOTIDES assumed defined elsewhere:\n",
        "# NUCLEOTIDE_MAP = {'-':0,'A':1,'C':2,'G':3,'T':4}\n",
        "# NUCLEOTIDES = {v:k for k,v in NUCLEOTIDE_MAP.items()}\n",
        "\n",
        "class AlignmentEnvironment:\n",
        "    def __init__(self, sequences, total_gap, max_n_seqs=None, max_msa_len=None):\n",
        "        \"\"\"\n",
        "        Column-based alignment environment.\n",
        "        At each step, the agent decides which sequences get a gap in the next aligned column.\n",
        "        \"\"\"\n",
        "        self.original_n_seqs = len(sequences)\n",
        "        self.max_n_seqs = max_n_seqs or len(sequences)\n",
        "        self.max_msa_len = max_msa_len or max(len(s) for s in sequences)\n",
        "\n",
        "        # Pad to equal length\n",
        "        sequences = self._pad_sequences(sequences, self.max_msa_len)\n",
        "        while len(sequences) < self.max_n_seqs:\n",
        "            sequences.append(\"A\" * self.max_msa_len)  # dummy padding row, won't affect alignment\n",
        "\n",
        "        self.initial_sequences = [list(s) for s in sequences]\n",
        "        self.total_gap = total_gap\n",
        "        self.initial_gap = total_gap\n",
        "        self.reset()\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad_sequences(sequences, target_len):\n",
        "        return [s.ljust(target_len, \"P\") for s in sequences]       # we can do another nucleotide for padding but not sure if we would want that for the embedding ???\n",
        "\n",
        "    # -------------------- ACTION SPACE --------------------\n",
        "    def get_valid_action_mask(self):\n",
        "        n = self.original_n_seqs\n",
        "        n_actions = 2 ** n\n",
        "        mask = np.ones(n_actions, dtype=bool)\n",
        "\n",
        "        # Ban the all-gap action (every bit = 1)\n",
        "        mask[n_actions - 1] = False\n",
        "        return mask\n",
        "\n",
        "\n",
        "    # ---------- CONSENSUS-FREE REWARD ----------\n",
        "    @staticmethod\n",
        "    def _pairwise_column_score(rows, match_reward=2.0, mismatch_penalty=-2.0, gap_penalty=-1.0):\n",
        "        \"\"\"\n",
        "        Computes the pairwise sum of scores for the given columns.\n",
        "        Each column is compared across all pairs of sequences.\n",
        "        \n",
        "        - Matches: +match_reward\n",
        "        - Mismatches: +mismatch_penalty\n",
        "        - Gaps ('-' or 'P'): +gap_penalty\n",
        "        \n",
        "        Args:\n",
        "            rows: List[List[str]] where rows[r][c] = nucleotide or gap symbol.\n",
        "        \"\"\"\n",
        "        if not rows:\n",
        "            return 0.0\n",
        "        \n",
        "        n = len(rows)\n",
        "        L = len(rows[0])\n",
        "        total_score = 0.0\n",
        "\n",
        "        for c in range(L):\n",
        "            # Take column across all sequences\n",
        "            col = [rows[r][c] for r in range(n)]\n",
        "            # Treat 'P' as '-'\n",
        "            col = ['-' if x == 'P' else x for x in col]\n",
        "\n",
        "            # Compare all sequence pairs in this column\n",
        "            for i in range(n):\n",
        "                for j in range(i + 1, n):\n",
        "                    a, b = col[i], col[j]\n",
        "                    if a == '-' or b == '-':\n",
        "                        total_score += gap_penalty\n",
        "                    elif a == b:\n",
        "                        total_score += match_reward\n",
        "                    else:\n",
        "                        total_score += mismatch_penalty\n",
        "\n",
        "        return float(total_score)\n",
        "\n",
        "\n",
        "    def get_current_state(self):\n",
        "        # pad each unaligned row to max_msa_len with 'P'\n",
        "        rows = []\n",
        "        for i in range(self.max_n_seqs):\n",
        "            rem = self.unaligned[i]\n",
        "            if len(rem) >= self.max_msa_len:\n",
        "                row = rem[:self.max_msa_len]\n",
        "            else:\n",
        "                row = rem + ['P'] * (self.max_msa_len - len(rem))\n",
        "            rows.append([NUCLEOTIDE_MAP.get(x, 0) for x in row])\n",
        "        # flatten R x C\n",
        "        flat = []\n",
        "        for r in rows:\n",
        "            flat.extend(r)\n",
        "        return flat\n",
        "\n",
        "    # Predicted-only metrics (SP/CS) for validation & optional reward shaping\n",
        "    def _predicted_sp_score(self, msa):\n",
        "        # +2 match (non-gap), -1 if any gap, -2 mismatch (non-gap)\n",
        "        n = len(msa)\n",
        "        if n == 0: return 0.0\n",
        "        L = len(msa[0])\n",
        "        assert all(len(s) == L for s in msa)\n",
        "        score = 0\n",
        "        from itertools import combinations\n",
        "        for i, j in combinations(range(n), 2):\n",
        "            si, sj = msa[i], msa[j]\n",
        "            for c in range(L):\n",
        "                a, b = si[c], sj[c]\n",
        "                if a == '-' or b == '-':\n",
        "                    score -= 1\n",
        "                elif a == b:\n",
        "                    score += 2\n",
        "                else:\n",
        "                    score -= 2\n",
        "        return float(score)\n",
        "\n",
        "    def _predicted_cs_fraction(self, msa):\n",
        "        n = len(msa)\n",
        "        if n == 0: return 0.0\n",
        "        L = len(msa[0])\n",
        "        assert all(len(s) == L for s in msa)\n",
        "        good = 0\n",
        "        for c in range(L):\n",
        "            col = [msa[r][c] for r in range(n)]\n",
        "            nz = [x for x in col if x != '-']\n",
        "            if nz and len(set(nz)) == 1:\n",
        "                good += 1\n",
        "        return good / L if L > 0 else 0.0\n",
        "\n",
        "    # Public calc for validation “reward-like” score\n",
        "    def calc_reward_from_alignment(self, aligned_list_of_str):\n",
        "        rows = [[ch for ch in row] for row in aligned_list_of_str[:self.original_n_seqs]]\n",
        "        return self._pairwise_column_score(rows)\n",
        "\n",
        "    def calc_reward(self):\n",
        "        \"\"\"Full alignment score of current state.\"\"\"\n",
        "        rows = [\"\".join(r) for r in self.aligned[:self.original_n_seqs]]\n",
        "        return self._pairwise_column_score([[ch for ch in row] for row in rows])\n",
        "\n",
        "    # -------------------- STEP --------------------\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one column alignment step.\n",
        "        Action encodes which sequences receive a gap this round.\n",
        "        \"\"\"\n",
        "        if self.done_flag:\n",
        "            return 0.0, self.get_current_state(), 1\n",
        "\n",
        "        n = self.original_n_seqs\n",
        "        bin_mask = [int(x) for x in bin(action)[2:].zfill(n)]  # e.g., 0b101 -> [1,0,1]\n",
        "\n",
        "        for i in range(n):\n",
        "            if bin_mask[i] == 1:\n",
        "                # insert gap\n",
        "                self.aligned[i].append('-')\n",
        "                self.total_gap -= 1\n",
        "            else:\n",
        "                # consume next nucleotide if available\n",
        "                if self.unaligned[i]:\n",
        "                    nt = self.unaligned[i].pop(0)\n",
        "                    self.aligned[i].append(nt)\n",
        "                else:\n",
        "                    # no bases left, add gap\n",
        "                    self.aligned[i].append('-')\n",
        "                    self.total_gap -= 1\n",
        "\n",
        "        # compute dense step reward\n",
        "        last_col_rows = [[self.aligned[i][-1]] for i in range(n)]  # n rows, length 1\n",
        "        reward = self._pairwise_column_score(last_col_rows)\n",
        "        reward = float(np.clip(reward, -5.0, 5.0))\n",
        "\n",
        "        # termination condition: all unaligned parts are empty or contain only 'P'\n",
        "        def _is_exhausted(seq):\n",
        "            return all(ch == 'P' for ch in seq) or len(seq) == 0\n",
        "        self.done_flag = all(_is_exhausted(u) for u in self.unaligned[:n])\n",
        "        done = int(self.done_flag)\n",
        "\n",
        "        return reward, self.get_current_state(), done\n",
        "\n",
        "    # -------- Reset / Permutation (unchanged) --------\n",
        "    def get_alignment(self):\n",
        "        alignment = []\n",
        "        for i in range(len(self.aligned)):\n",
        "            alignment.append(''.join([NUCLEOTIDES[self.aligned[i][j]] for j in range(len(self.aligned[i]))]))\n",
        "        return alignment\n",
        "\n",
        "    def get_original_alignment(self):\n",
        "        max_len = max(len(r) for r in self.aligned)\n",
        "        out = []\n",
        "        for i in range(self.original_n_seqs):\n",
        "            row = self.aligned[i] + ['-'] * (max_len - len(self.aligned[i]))\n",
        "            out.append(''.join(row))\n",
        "        return out\n",
        "\n",
        "\n",
        "    # -------------------- STATE / RESET --------------------\n",
        "    def reset(self):\n",
        "        self.unaligned = [list(seq) for seq in self.initial_sequences]\n",
        "        self.aligned = [[] for _ in range(self.max_n_seqs)]\n",
        "        self.total_gap = self.initial_gap\n",
        "        self.done_flag = False\n",
        "        return self.get_current_state()\n",
        "\n",
        "    def randomize_sequence_order(self, apply=True):\n",
        "        if not apply or self.original_n_seqs <= 1:\n",
        "            self.seq_permutation = None\n",
        "            self.seq_permutation_inv = None\n",
        "            return\n",
        "        self.seq_permutation = np.random.permutation(self.original_n_seqs)\n",
        "        self.seq_permutation_inv = np.argsort(self.seq_permutation)\n",
        "        self.original[:self.original_n_seqs]        = [self.original[i]        for i in self.seq_permutation]\n",
        "        self.sequences[:self.original_n_seqs]       = [self.sequences[i]       for i in self.seq_permutation]\n",
        "        self.sep_nuc_in_seq[:self.original_n_seqs]  = [self.sep_nuc_in_seq[i]  for i in self.seq_permutation]\n",
        "        self.label_encoded_seqs[:self.original_n_seqs] = [self.label_encoded_seqs[i] for i in self.seq_permutation]\n",
        "        gaps_copy = self.gaps_per_sequence[:self.original_n_seqs].copy()\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            self.gaps_per_sequence[new_idx] = gaps_copy[old_idx]\n",
        "\n",
        "    \n",
        "    def get_unpermuted_gaps(self):\n",
        "        if self.seq_permutation_inv is None:\n",
        "            return self.gaps_per_sequence[:self.original_n_seqs]\n",
        "        \n",
        "        original_order = [0] * self.original_n_seqs\n",
        "        for new_idx, old_idx in enumerate(self.seq_permutation):\n",
        "            original_order[old_idx] = self.gaps_per_sequence[new_idx]\n",
        "        return original_order\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_sp_score(pred, match=2, mismatch=-2, gap = -1):\n",
        "        n = len(pred)\n",
        "        L = len(pred[0])\n",
        "\n",
        "        score = 0\n",
        "        for i, j in combinations(range(n), 2):\n",
        "            si, sj = pred[i], pred[j]\n",
        "            for c in range(L):\n",
        "                a, b = si[c], sj[c]\n",
        "                if a == '-' and b == '-':\n",
        "                    score += 0 \n",
        "                elif a == '-' or b == '-':\n",
        "                    score += gap\n",
        "                elif a == b:\n",
        "                    score += match\n",
        "                else:\n",
        "                    score += mismatch\n",
        "        return float(score)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_cs_score(pred):\n",
        "        n = len(pred)\n",
        "        L = len(pred[0])\n",
        "\n",
        "        good = 0\n",
        "        for c in range(L):\n",
        "            col = [pred[r][c] for r in range(n)]\n",
        "            nz = [x for x in col if x != '-']      # non-gap residues\n",
        "            if len(nz) == 0:\n",
        "                # no residues in this column → by definition not a “matching residues” column\n",
        "                continue\n",
        "            if len(set(nz)) == 1:\n",
        "                good += 1\n",
        "        return good / L if L > 0 else 0.0\n",
        "    \n",
        "    @staticmethod\n",
        "    def compute_alignment_metrics(pred, ref):\n",
        "        \"\"\"\n",
        "        Computes predicted-only scores (SP, CS) and reference-based metrics (Q*, TC*).\n",
        "\n",
        "        Q(A,R): pair-based accuracy (sum-of-pairs)\n",
        "        TC(A,R): column-based accuracy (total-column match)\n",
        "\n",
        "        Columns are compared by tuples (bases_across_sequences, column_index)\n",
        "        so the logic is parallel to the pair-based comparison.\n",
        "        \"\"\"\n",
        "        assert len(pred) == len(ref), \"Pred/ref must have same number of sequences.\"\n",
        "        n = len(ref)\n",
        "        if n == 0:\n",
        "            return {k: 0.0 for k in [\n",
        "                'pred_sp','pred_cs','Q_acc','Q_prec','Q_rec','Q_f1',\n",
        "                'TC_acc','TC_prec','TC_rec','TC_f1'\n",
        "            ]}\n",
        "        Lp = len(pred[0])\n",
        "        Lr = len(ref[0])\n",
        "        assert all(len(s) == Lp for s in pred)\n",
        "        assert all(len(s) == Lr for s in ref)\n",
        "        L = min(Lp, Lr)\n",
        "\n",
        "        # --- predicted-only metrics\n",
        "        pred_sp = AlignmentEnvironment.get_sp_score(pred)\n",
        "        pred_cs = AlignmentEnvironment.get_cs_score(pred)\n",
        "\n",
        "        # ---------- Q (pair) metrics ----------\n",
        "        def get_pairs(msa):\n",
        "            pairs = set()\n",
        "            for i, j in combinations(range(n), 2):\n",
        "                seq_i, seq_j = msa[i], msa[j]\n",
        "                for c in range(L):\n",
        "                    a, b = seq_i[c], seq_j[c]\n",
        "                    if a != '-' and b != '-':\n",
        "                        pairs.add((i, j, c))\n",
        "            return pairs\n",
        "\n",
        "        pred_pairs = get_pairs(pred)\n",
        "        ref_pairs = get_pairs(ref)\n",
        "\n",
        "        TPp = len(pred_pairs & ref_pairs)\n",
        "        FPp = len(pred_pairs - ref_pairs)\n",
        "        FNp = len(ref_pairs - pred_pairs)\n",
        "\n",
        "        Q_acc  = TPp / len(ref_pairs) if len(ref_pairs) > 0 else 0.0\n",
        "        Q_prec = TPp / (TPp + FPp) if (TPp + FPp) > 0 else 0.0\n",
        "        Q_rec  = TPp / (TPp + FNp) if (TPp + FNp) > 0 else 0.0\n",
        "        Q_f1   = (2 * Q_prec * Q_rec / (Q_prec + Q_rec)) if (Q_prec + Q_rec) > 0 else 0.0\n",
        "\n",
        "        # ---------- TC (column) metrics ----------\n",
        "        def get_columns(msa):\n",
        "            cols = set()\n",
        "            for c in range(L):\n",
        "                col = tuple(msa[r][c] for r in range(n))\n",
        "                cols.add((c, col))  # include index for uniqueness\n",
        "            return cols\n",
        "\n",
        "        pred_cols = get_columns(pred)\n",
        "        ref_cols  = get_columns(ref)\n",
        "\n",
        "        TPc = len(pred_cols & ref_cols)\n",
        "        FPc = len(pred_cols - ref_cols)\n",
        "        FNc = len(ref_cols - pred_cols)\n",
        "\n",
        "        TC_acc  = TPc / len(ref_cols) if len(ref_cols) > 0 else 0.0\n",
        "        TC_prec = TPc / (TPc + FPc) if (TPc + FPc) > 0 else 0.0\n",
        "        TC_rec  = TPc / (TPc + FNc) if (TPc + FNc) > 0 else 0.0\n",
        "        TC_f1   = (2 * TC_prec * TC_rec / (TC_prec + TC_rec)) if (TC_prec + TC_rec) > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            \"pred_sp\": float(pred_sp),\n",
        "            \"pred_cs\": float(pred_cs),\n",
        "            \"Q_acc\": float(Q_acc),\n",
        "            \"Q_prec\": float(Q_prec),\n",
        "            \"Q_rec\": float(Q_rec),\n",
        "            \"Q_f1\": float(Q_f1),\n",
        "            \"TC_acc\": float(TC_acc),\n",
        "            \"TC_prec\": float(TC_prec),\n",
        "            \"TC_rec\": float(TC_rec),\n",
        "            \"TC_f1\": float(TC_f1)\n",
        "        }\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## never used?? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_prediction_episode(env, agent, expected_gaps, max_factor=1.25):\n",
        "    import math\n",
        "    import numpy as np\n",
        "    \n",
        "    state = env.reset()\n",
        "    max_steps = max(1, int(math.ceil(expected_gaps * max_factor)))\n",
        "    steps, rewards = 0, []\n",
        "    \n",
        "    for t in range(max_steps):\n",
        "        valid_mask = env.get_valid_action_mask().astype(bool)\n",
        "        \n",
        "        q_values = agent.forward(state)\n",
        "        action = masked_epsilon_greedy(\n",
        "            q_values=q_values,\n",
        "            valid_mask=valid_mask,\n",
        "            epsilon=getattr(agent, \"epsilon\", 0.1),\n",
        "        )\n",
        "        \n",
        "        reward, next_state, done = env.step(action)\n",
        "        rewards.append(float(reward))\n",
        "        \n",
        "        if hasattr(agent, \"replay_memory\"):\n",
        "            agent.replay_memory.store(\n",
        "                (state, next_state, action, reward, bool(done), valid_mask.copy())\n",
        "            )\n",
        "        \n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        \n",
        "        if done == 1 or not valid_mask.any():\n",
        "            break\n",
        "    \n",
        "    gaps_dist = env.get_unpermuted_gaps() if hasattr(env, \"get_unpermuted_gaps\") \\\n",
        "                else env.gaps_per_sequence[:env.original_n_seqs]\n",
        "    \n",
        "    return {\n",
        "        \"steps\": steps,\n",
        "        \"avg_reward\": float(np.mean(rewards)) if rewards else 0.0,\n",
        "        \"sum_reward\": float(np.sum(rewards)) if rewards else 0.0,\n",
        "        \"gaps_distribution\": gaps_dist,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'start': ['TACTACAGTTCTTAAAAATAATCTATTAAAATTTTTTTGCT', 'TAGTACGATTCGTGAAAATAATCTGTTAAAATTCTTTTTCT', 'TTATACAATTTTTTGAGGATTAATCTGTTGAAATTATTGTTCT'], 'solution': ['TACTACAGTTCTT--AAAAATAATCTATTAAAATTTTTTTGCT', 'TAGTACGATTCGT--GAAAATAATCTGTTAAAATTCTTTTTCT', 'TTATACAATTTTTTGAGGATTAATCTGTTGAAATTATTGTTCT'], 'n_gaps': 4, 'moves': [-1, -1, -1, -1], 'n_sequences': 3, 'idx': 0}\n"
          ]
        }
      ],
      "source": [
        "def _count_inserted_gaps_from_sequences(start, solution):\n",
        "    dash_start = sum(str(s).count('-') for s in start)\n",
        "    dash_solution = sum(str(s).count('-') for s in solution)\n",
        "    return max(0, dash_solution - dash_start)\n",
        "\n",
        "def convert_column_major_solution(msa_string, n_seq):\n",
        "    \"\"\"\n",
        "    Converts a column-major MSA string (down columns first) into\n",
        "    a row-major list of aligned sequences.\n",
        "    \n",
        "    Args:\n",
        "        msa_string (str): e.g. \"AAACC---CGGTTTT\"\n",
        "        n_seq (int): number of sequences (rows)\n",
        "    \n",
        "    Returns:\n",
        "        list[str]: e.g. [\"ACGT-\", \"A-GT-\", \"AC-T-\"]\n",
        "    \"\"\"\n",
        "    if not msa_string or n_seq <= 0:\n",
        "        return []\n",
        "\n",
        "    # Split into chunks of n_seq (each chunk = one column)\n",
        "    columns = [msa_string[i:i+n_seq] for i in range(0, len(msa_string), n_seq)]\n",
        "\n",
        "    # Transpose columns -> rows\n",
        "    seqs = [''.join(col[i] for col in columns) for i in range(n_seq)]\n",
        "    return seqs\n",
        "\n",
        "def convert_huggingface_to_samples(dataset, max_samples=None):\n",
        "    samples = []\n",
        "    for i, ex in enumerate(dataset):\n",
        "        if max_samples and i >= max_samples:\n",
        "            break\n",
        "\n",
        "        unaligned_seqs = ex.get('unaligned_seqs', {})\n",
        "        MSA = ex.get('MSA', \"\")\n",
        "\n",
        "        if not unaligned_seqs or not MSA:\n",
        "            continue\n",
        "\n",
        "        start = [unaligned_seqs[k] for k in sorted(unaligned_seqs.keys())]\n",
        "        n_seq = len(start)\n",
        "        solution = convert_column_major_solution(MSA, n_seq)\n",
        "\n",
        "        accepted_pairs = [(str(a), str(b)) for a, b in combinations(range(len(start)), 2)]\n",
        "        n_gaps = _count_inserted_gaps_from_sequences(start, solution)\n",
        "\n",
        "        sample = {\n",
        "            'start': start,\n",
        "            'solution': solution,\n",
        "            'n_gaps': n_gaps,\n",
        "            'moves': [-1] * n_gaps,  # keep list length equal to n_gaps as this is never actually used in the DQN\n",
        "            'n_sequences': len(start),\n",
        "            'idx': i\n",
        "        }\n",
        "        samples.append(sample)\n",
        "    return samples\n",
        "\n",
        "\n",
        "def filter_by_seq_length(example, max_len=MAX_MSA_LEN):\n",
        "    \"\"\"Keep only samples where every unaligned sequence is <= max_len.\"\"\"\n",
        "    if \"unaligned_seqs\" not in example:\n",
        "        return False\n",
        "    seqs = example[\"unaligned_seqs\"].values() if isinstance(example[\"unaligned_seqs\"], dict) else example[\"unaligned_seqs\"]\n",
        "    return all(len(seq) <= max_len for seq in seqs)\n",
        "\n",
        "# --- Load and filter datasets ---\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"train\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "train_samples = convert_huggingface_to_samples(ds, max_samples=training_size)\n",
        "print(train_samples[0])\n",
        "\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"validation\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "val_samples = convert_huggingface_to_samples(ds)\n",
        "\n",
        "ds = load_dataset(\"dotan1111/MSA-nuc-3-seq\", split=\"test\")\n",
        "ds = ds.filter(filter_by_seq_length)\n",
        "test_samples = convert_huggingface_to_samples(ds) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Dimension Analysis (Using ALL Data with Padding) DEPRECATEDDDDDDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_n_seqs = max(s['n_sequences'] for s in train_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in train_samples)\n",
        "\n",
        "for sample in train_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n",
        "\n",
        "max_n_seqs = max(s['n_sequences'] for s in val_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in val_samples)\n",
        "\n",
        "for sample in val_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n",
        "\n",
        "max_n_seqs = max(s['n_sequences'] for s in test_samples)\n",
        "max_cons_len = max(s['consensus_length'] for s in test_samples)\n",
        "for sample in test_samples:\n",
        "    sample['original_n_seqs'] = sample['n_sequences']\n",
        "    sample['original_cons_len'] = sample['consensus_length']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training with Variable Dimensions (Using Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:   0%|          | 0/5 [00:00<?, ?it/s]/var/folders/np/3n8gblwj5054scn97qw91dpc0000gn/T/ipykernel_44970/1541028366.py:106: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  batch_next_mask = torch.BoolTensor(next_mask).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Episode 1] Initialized with 76 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  20%|██        | 1/5 [00:09<00:39,  9.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 77 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    AGTATATAGGACTTCTCTGCGTTACTCTTGTATACCTA\n",
            "    AGTATATAATTTTCGGCGTCACTTTTGTATACCTA\n",
            "    AGTATACATTATTCTACGTCAATTTTGTATACCTA\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    AG--TA---TATA--G--GACTTC----T--C------TG-CGTTAC---T-CT--T--G-----T-A--TACC-TA\n",
            "    --AGT--AT-A--TAATT---TTCGGCG-TCACTTTT-G-TA--T-A-CCTAP-P-P-P-PP---P-PPP-P-P---\n",
            "    ---AGTATA-CA-TT-AT-T--------C-TA-CGTCA-A--TTT--T-GTAT-AC-CT--APPPPP----PPPP-P\n",
            "   Episode Reward: -209.000 | Epsilon: 0.900\n",
            "\n",
            "\n",
            "[Episode 2] Initialized with 84 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  40%|████      | 2/5 [00:51<01:24, 28.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 76 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    TGGTTCCACATTTAGTTTAGATGGCTTTTACTTCATAGCTTC\n",
            "    TGGTTTCACATTTTATTTAGATTGATTTTATTTCATAAATTT\n",
            "    TGATGCCACATTCAACTAAGATTGTTTTTATTTCATAATTTT\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    T--GGTT--CCACAT-T-T-A-GT------TTAG--ATG-GCTT-TTA--C-TTCAT-AG--CT-TCP--PPP-PP\n",
            "    T-G----GTT-TC---ACATT-T-TATT-T-AGAT--TGA-T-TTTA-TTT-CA--TA---AATTT---P-PPPPP\n",
            "    -T-GA------T---G----CCAC-A-TT---CAAC-------T-AA-G--ATTGTTTT-TAT-T-TCATAATTTT\n",
            "   Episode Reward: -196.000 | Epsilon: 0.900\n",
            "\n",
            "\n",
            "[Episode 3] Initialized with 78 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  60%|██████    | 3/5 [01:29<01:05, 32.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 73 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    GTACTTAGACACGGCAATCTATCGGTCAGCGTTGGAAAG\n",
            "    GTACTAAGGATAACCAGTTACTAGCAAACATTGAA\n",
            "    GTGCTTAAACGCGACGAACGACCGGTTAGCGTTGAAGTT\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    --GTACT--TAGA-C-A--CGG-CAAT-C---TAT-CGGT-CAGC-G--TT--GGA-AA---G-P--PPPP-P\n",
            "    GTA--C-TA--A--GGAT--AA-C--CAGT-TAC-T-AG-CA-A----A-CAT-TGAAPPP-PPP-P-PPPP-\n",
            "    -G-TG-C--TT-AAA--CG---CG--AC-GA-AC---G----ACCG-GTT-A-GC--GT-TG-A-A--G--TT\n",
            "   Episode Reward: -206.000 | Epsilon: 0.900\n",
            "\n",
            "\n",
            "[Episode 4] Initialized with 78 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:  80%|████████  | 4/5 [02:07<00:34, 34.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 73 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    TATAAGTTAATTAATGGAGTTTAGTTTCAAGACTGTTAA\n",
            "    TATAAGTTAACTGATGGAGTTGGGTTTCAAGACTGTTAA\n",
            "    TACAAGGTAATTAATGGAGTTGGGTTTCAAGACTGTAAA\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    T---ATA-A--GTT-AA--TT-A-A---TGG--AGTT-T-----AGTT--TCAAG-ACTGT-T---AAPP---\n",
            "    TATA---AG-T--T----A-A---CT-GA-TG---GA--G-TT---GGGTTT---CA-A--GACTGT--TA-A\n",
            "    TA-C----AAGG-TA--A-T-T-AATG-GA-GT-TGGGT-T--TCA-AG--ACT--GTA----AAP-P--PP-\n",
            "   Episode Reward: -198.000 | Epsilon: 0.900\n",
            "\n",
            "\n",
            "[Episode 5] Initialized with 70 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 5/5 [02:40<00:00, 32.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 64 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    CTCTACAGTCTAACG\n",
            "    TTATGTGGTATAATGATTCATTAGTATAAATTATA\n",
            "    TTATATAATCTAAGAGTTCACTATTACAGTTTATG\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    CTCTACAG----TCT--AAC-G--P---PP-PP-PPPPPP--PP-PPPP-PPPPP--PP-PPPP\n",
            "    -TTA-T-G-TG-G-TAT-A-ATGATT-C-AT---TA---G-T--A----TA-AA----TTAT-A\n",
            "    --TTATATA--ATCT-AA-GA-GTTCACTAT-TAC--AG-TT-T---ATGP-P--PPP---P--\n",
            "   Episode Reward: -147.000 | Epsilon: 0.900\n",
            "\n",
            "\n",
            "Epoch 1 Summary: reward=-191.20, memory=363, ε=0.900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Validation: SP=-93.60, CS=0.111, Q_acc=0.563, Q_prec=0.851, Q_rec=0.563, Q_f1=0.677, TC_acc=0.080, TC_prec=0.080, TC_rec=0.080, TC_f1=0.080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Episode 1] Initialized with 74 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  20%|██        | 1/5 [00:36<02:26, 36.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 69 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    CCTTCCCGACCCAAACCTCGAGAATTTATCAGAACAT\n",
            "    CTTTTTACTCATGTCTTGAGAATTAATCCGAAGAT\n",
            "    CTTTTTGCCCAAATCTTGAGGATTAGTGAAT\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    -CC-----T--T-CCC-G-A--C--CCA-AACCTC----G-A-GAA-TTT--A-T-C-AG-A-A-C-AT\n",
            "    C-TTT-T--TA-CT-CA--T----G--T-CTT-GA-G-A---AT-------T-AATCC-GAAGATPPP-\n",
            "    --C--T-T-TT-T--G--C-CC-AA--AT-C----TTG-AGGAT-TAGTGA-A--TPP-PPP-PPP---\n",
            "   Episode Reward: -194.000 | Epsilon: 0.899\n",
            "\n",
            "\n",
            "[Episode 2] Initialized with 70 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  40%|████      | 2/5 [01:11<01:45, 35.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 67 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    TAATTTTGTAGTAAAAATAAAATTTAGTGAAGCTC\n",
            "    CAACTTCATAGTAACATTAAAATTA\n",
            "    CAACTTCATAGTAACATTAAAATTA\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    -TA--A-T-T-TTG-T---A-GT-AAAA-ATA--AA----A-T-T-T-AG------T-GAAG-C-TC\n",
            "    C---A--AC-TT--C--A-TA--G--T-AACATT-A--AA--A-TT-A-PP-PPPPPPP----P---\n",
            "    C-AAC-T---T-C---ATA-G-TAACA-T--T-A-AAAT-TAPPP----P-PP-PP---P--PPP-P\n",
            "   Episode Reward: -181.000 | Epsilon: 0.899\n",
            "\n",
            "\n",
            "[Episode 3] Initialized with 72 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  60%|██████    | 3/5 [01:49<01:13, 36.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 72 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    CAGATTTTTAGATGTATAAAGAGTGCGACCCTGTTA\n",
            "    TAAATTTTTTAATGTTTGAAGAATGCGCCCCTACTA\n",
            "    AAAATGTGTATATGTATTGGTAATGCGCCCCCGTTA\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    CAG-A--T---T--T-TTA-GA--TGTAT--AA-AGAGT--G-CGA--CC-CTGTT-APPP-PPPP-PP---\n",
            "    --T---AA---AT-TTTT-----T-AA--TGTT-TGA--AGAA---TGC-G---C------CCCTA--CT-A\n",
            "    -A-AAA--TGT-GT-ATA-T--GTA--T---TGGT--A--A--T--GCG-C--CC-C-C--GTTA-PP--P-\n",
            "   Episode Reward: -208.000 | Epsilon: 0.899\n",
            "\n",
            "\n",
            "[Episode 4] Initialized with 70 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:  80%|████████  | 4/5 [02:23<00:35, 35.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 65 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    TGTCGCTTTCGTAGGCATTTTATTTTCGGTATGAA\n",
            "    TGCCGTCTGAATAGGACTTCATATTCGATGTAGA\n",
            "    TATTGCTTTCATAGGGTTTTATTTTCGGTGTAAA\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    T-GTC-GC-T--TT-C-GTA--G--GCA-TTTTAT-------TTT---C--GG-TAT--G-A--A\n",
            "    -T-G-C-C-GT-CTG-AAT--AGGAC-TTCA-TATTCGATGTAG-A-PPPP-P-P-PPPPPP-PP\n",
            "    TAT-TG-CTT-TCATA---GGGT--TTTA--TTTT-C-G-GTGTAAA---P-PP-P---P-PP-P\n",
            "   Episode Reward: -163.000 | Epsilon: 0.899\n",
            "\n",
            "\n",
            "[Episode 5] Initialized with 72 gaps, max_n_seqs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 5/5 [02:56<00:00, 35.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated early after 60 steps (unaligned part empty).\n",
            "\n",
            "   Original Sequences:\n",
            "    GCAAGAATATGCTTCTTTGGGGTTTAAATAAACTTT\n",
            "    AAAAAAGTATTCTACTTGAGGGTCAGGGTAAATCTT\n",
            "    ACAAGAGTATTTTGCTTGGAAATAAAGATGAATTTT\n",
            "\n",
            "   Final Aligned Sequences (Predicted):\n",
            "    -GC--A-AGA-ATATGC-TTC---TTTGGGGTT-TA-A-AT-AAA-CTTTPP-PPPPP--\n",
            "    AA--AAAA-GT-AT--TC--TA--CTT-GA-G-G--G-T--CAGGGT-A-AA---TC-TT\n",
            "    ACAAGAGTATT---T------TGCTTG-GA-AA---TAAAG---ATG-A-A-TTT-TP-P\n",
            "   Episode Reward: -153.000 | Epsilon: 0.899\n",
            "\n",
            "\n",
            "Epoch 2 Summary: reward=-179.80, memory=696, ε=0.899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Validation: SP=-88.40, CS=0.179, Q_acc=0.668, Q_prec=0.935, Q_rec=0.668, Q_f1=0.774, TC_acc=0.085, TC_prec=0.085, TC_rec=0.085, TC_f1=0.085\n",
            "\n",
            "✅ Training complete.\n",
            "Model saved to: ../result/agent/2025-11-12_12-06-31_model.pt\n",
            "Training log saved to: ../result/log/2025-11-12_12-06-31_log.csv\n"
          ]
        }
      ],
      "source": [
        "# --- Create results directory ---\n",
        "os.makedirs(\"../result/log\", exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "log_path = os.path.join(\"../result/log\", f\"{timestamp}_log.csv\")\n",
        "\n",
        "# --- Prepare CSV logging ---\n",
        "log_fields = [\n",
        "    \"epoch\", \"avg_train_reward\", \"epsilon\",\n",
        "    \"SP\", \"CS\", \"Q_acc\", \"Q_prec\", \"Q_rec\", \"Q_f1\", \"TC_acc\", \"TC_prec\", \"TC_rec\", \"TC_f1\"\n",
        "]\n",
        "\n",
        "with open(log_path, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=log_fields)\n",
        "    writer.writeheader()\n",
        "\n",
        "# --- Agent setup ---\n",
        "num_seqs = max_n_seqs  # e.g. 3\n",
        "agent = DQNAgent(\n",
        "    action_number=2 ** num_seqs,   # <-- key fix\n",
        "    num_seqs=num_seqs,\n",
        "    max_grid=MAX_MSA_LEN,\n",
        "    max_value=MAX_MSA_LEN * 100,\n",
        "    epsilon=0.9,\n",
        "    delta=0.01,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    learning_rate=0.001,\n",
        "    memory_size=5000\n",
        ")\n",
        "\n",
        "epochs = 2   # 10\n",
        "samples_per_epoch = 5  # 100\n",
        "val_samples_per_epoch = 5  # 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_samples = random.sample(train_samples, min(len(train_samples), samples_per_epoch))\n",
        "    epoch_rewards = []\n",
        "\n",
        "    for sample_idx, sample in enumerate(tqdm(epoch_samples, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "        env = AlignmentEnvironment(\n",
        "            sequences=sample[\"start\"],\n",
        "            total_gap=get_expected_gaps(sample),\n",
        "            max_n_seqs=max_n_seqs,\n",
        "            max_msa_len=MAX_MSA_LEN\n",
        "        )\n",
        "        print(f\"\\n[Episode {sample_idx+1}] Initialized with {get_expected_gaps(sample)} gaps, max_n_seqs={max_n_seqs}\")\n",
        "\n",
        "        state = env.reset()\n",
        "        # env.randomize_sequence_order(apply=True)\n",
        "\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "        max_steps = get_expected_gaps(sample) * 5\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "\n",
        "        # --- Episode rollout ---\n",
        "        while step_count < max_steps:\n",
        "            #print(f\"\\n Step {step_count+1}\")\n",
        "            #for i, u in enumerate(env.unaligned[:env.original_n_seqs]):\n",
        "                #print(f\"   Seq{i+1} unaligned: {''.join(u) if u else '(empty)'}\")\n",
        "\n",
        "            action = agent.select_action(state, valid_action_mask=valid_mask)\n",
        "            #print(f\"   → Action chosen: {bin(action)[2:].zfill(env.original_n_seqs)}\")\n",
        "\n",
        "            reward, next_state, done = env.step(action)\n",
        "\n",
        "            next_valid_mask = env.get_valid_action_mask()\n",
        "            agent.replay_memory.store((state, next_state, action, reward, done, next_valid_mask))\n",
        "\n",
        "            agent.update()\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "            state = next_state\n",
        "            valid_mask = next_valid_mask\n",
        "\n",
        "            # print(f\"   Reward: {reward:.3f}, Done: {done}, Remaining unaligned lengths: {[len(u) for u in env.unaligned[:env.original_n_seqs]]}\")\n",
        "\n",
        "            if done == 1:\n",
        "                print(f\"Terminated early after {step_count} steps (unaligned part empty).\")\n",
        "                break\n",
        "\n",
        "        agent.update_epsilon()\n",
        "        epoch_rewards.append(episode_reward)\n",
        "\n",
        "        # --- Print original vs. aligned sequences ---\n",
        "        print(\"\\n   Original Sequences:\")\n",
        "        for s in sample[\"start\"]:\n",
        "            print(\"   \", s)\n",
        "\n",
        "        print(\"\\n   Final Aligned Sequences (Predicted):\")\n",
        "        final_alignment = env.get_original_alignment()\n",
        "        for s in final_alignment:\n",
        "            print(\"   \", s)\n",
        "\n",
        "        print(f\"   Episode Reward: {episode_reward:.3f} | Epsilon: {agent.current_epsilon:.3f}\\n\")\n",
        "\n",
        "    # --- Epoch summary ---\n",
        "    avg_train_reward = np.mean(epoch_rewards)\n",
        "    print(f\"\\nEpoch {epoch+1} Summary: reward={avg_train_reward:.2f}, memory={agent.replay_memory.size}, ε={agent.current_epsilon:.3f}\")\n",
        "\n",
        "    # --- Validation ---\n",
        "    val_metrics_all = {\n",
        "        \"pred_sp\": [], \"pred_cs\": [],\n",
        "        \"Q_acc\": [], \"Q_prec\": [], \"Q_rec\": [], \"Q_f1\": [],\n",
        "        \"TC_acc\": [], \"TC_prec\": [], \"TC_rec\": [], \"TC_f1\": []\n",
        "    }\n",
        "\n",
        "    for val_sample in random.sample(val_samples, min(val_samples_per_epoch, len(val_samples))):\n",
        "        val_env = AlignmentEnvironment(\n",
        "            sequences=val_sample[\"start\"],\n",
        "            total_gap=get_expected_gaps(val_sample),\n",
        "            max_n_seqs=max_n_seqs,\n",
        "            max_msa_len=MAX_MSA_LEN\n",
        "        )\n",
        "\n",
        "        predicted, _ = run_dqn_inference(agent, val_env, get_expected_gaps(val_sample))\n",
        "        metrics = val_env.compute_alignment_metrics(predicted, val_sample[\"solution\"])\n",
        "        val_reward = val_env.calc_reward()\n",
        "\n",
        "        for k, v in metrics.items():\n",
        "            val_metrics_all[k].append(v)\n",
        "\n",
        "    avg_val_metrics = {k: np.mean(v) if len(v) > 0 else 0.0 for k, v in val_metrics_all.items()}\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} Validation: \"\n",
        "        f\"SP={avg_val_metrics['pred_sp']:.2f}, \"\n",
        "        f\"CS={avg_val_metrics['pred_cs']:.3f}, \"\n",
        "        f\"Q_acc={avg_val_metrics['Q_acc']:.3f}, \"\n",
        "        f\"Q_prec={avg_val_metrics['Q_prec']:.3f}, \"\n",
        "        f\"Q_rec={avg_val_metrics['Q_rec']:.3f}, \"\n",
        "        f\"Q_f1={avg_val_metrics['Q_f1']:.3f}, \"\n",
        "        f\"TC_acc={avg_val_metrics['TC_acc']:.3f}, \"\n",
        "        f\"TC_prec={avg_val_metrics['TC_prec']:.3f}, \"\n",
        "        f\"TC_rec={avg_val_metrics['TC_rec']:.3f}, \"\n",
        "        f\"TC_f1={avg_val_metrics['TC_f1']:.3f}\"\n",
        "    )\n",
        "\n",
        "    # --- Log to CSV ---\n",
        "    with open(log_path, \"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=log_fields)\n",
        "        writer.writerow({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"avg_train_reward\": avg_train_reward,\n",
        "            \"epsilon\": agent.current_epsilon,\n",
        "            \"SP\": avg_val_metrics[\"pred_sp\"],\n",
        "            \"CS\": avg_val_metrics[\"pred_cs\"],\n",
        "            \"Q_acc\": avg_val_metrics[\"Q_acc\"],\n",
        "            \"Q_prec\": avg_val_metrics[\"Q_prec\"],\n",
        "            \"Q_rec\": avg_val_metrics[\"Q_rec\"],\n",
        "            \"Q_f1\": avg_val_metrics[\"Q_f1\"],\n",
        "            \"TC_acc\": avg_val_metrics[\"TC_acc\"],\n",
        "            \"TC_prec\": avg_val_metrics[\"TC_prec\"],\n",
        "            \"TC_rec\": avg_val_metrics[\"TC_rec\"],\n",
        "            \"TC_f1\": avg_val_metrics[\"TC_f1\"]\n",
        "        })\n",
        "\n",
        "# --- Save model ---\n",
        "os.makedirs(\"../result/agent\", exist_ok=True)\n",
        "model_path = os.path.join(\"../result/agent\", f\"{timestamp}_model.pt\")\n",
        "agent.save_model(model_path)\n",
        "\n",
        "print(f\"\\n Training complete.\")\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Training log saved to: {log_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_dqn_inference(agent, env, expected_gaps, max_steps=None):\n",
        "    \"\"\"\n",
        "    Runs an inference episode using the trained DQN agent in the consensus-free environment.\n",
        "    \"\"\"\n",
        "    if max_steps is None:\n",
        "        max_steps = expected_gaps * 2\n",
        "\n",
        "    state = env.reset()\n",
        "    # env.randomize_sequence_order(apply=True)\n",
        "\n",
        "    actions_taken = []\n",
        "    valid_mask = env.get_valid_action_mask()\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if valid_mask.sum() == 0:\n",
        "            break\n",
        "\n",
        "        # Predict next action\n",
        "        action = agent.predict(state, valid_action_mask=valid_mask)\n",
        "\n",
        "        # Convert linear action index to (sequence, position)\n",
        "        seq_idx = action // env.max_msa_len\n",
        "        pos = action % env.max_msa_len\n",
        "        actions_taken.append((seq_idx, pos))\n",
        "\n",
        "        # Apply action in environment\n",
        "        _, next_state, done = env.step(action)\n",
        "        state = next_state\n",
        "        valid_mask = env.get_valid_action_mask()\n",
        "\n",
        "        if done == 1:\n",
        "            break\n",
        "\n",
        "    predicted = env.get_original_alignment()\n",
        "\n",
        "    # Clean up padding ('P') — treat it as gap ('-') for clarity\n",
        "    predicted = [seq.replace('P', '-') for seq in predicted[:env.original_n_seqs]]\n",
        "\n",
        "    return predicted, actions_taken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "COMP545",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
